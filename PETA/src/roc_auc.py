# -*- coding: utf-8 -*-
from __future__ import print_function, division
"""MuestraFull3700_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dR7ZG17_Mmr3y3JV0mLf12Bp8U8J27Aa
"""

try:
    from google.colab import drive
    drive.mount('/content/gdrive')
except ModuleNotFoundError:
    print('Not running on Google')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import os
import time
import copy
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms, utils, models, datasets
import torch.nn as nn
import torch.optim as optim
import nibabel as nib
import scipy.ndimage as ndi
from pathlib import Path
from PIL import Image
import io
import json
import random
import sklearn.metrics
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
import sys
import json

from util import collectAllData

# Source: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/
def calculateAUCROCs(testY, predY, experimentOutputFolder, experimentName, validationSet, labels = [0,1,2], labelDict = {
    0: "CN",
    1: "AD",
    2: "MCI"
  }, epoch = 0, executionNumber = 0):


  #print(testY)
  #print(predY)

  ns_probs = np.empty( shape = predY.shape )
  ns_probs[:, :] = 1.0/len(labels)

  # calculate scores
  ns_auc = roc_auc_score(testY, ns_probs, multi_class='ovr', labels= labels, average = None)
  lr_auc = roc_auc_score(testY, predY, multi_class='ovr', labels= labels, average = None)

  f = open(os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch)  + '_' + validationSet + '_roc_stats.txt'), "w")
  f.write("AUC: " + str(lr_auc) + "\n")
  f.close()
  
  # Por separado
  for i in labels:
    ns_fpr, ns_tpr, _ = roc_curve(testY, ns_probs[:, i], pos_label = i) 
    lr_fpr, lr_tpr, _ = roc_curve(testY, predY[:, i], pos_label = i)

    auc = lr_auc[i]

    # plot the roc curve for the model
    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No ' + labelDict[i])
    pyplot.plot(lr_fpr, lr_tpr, marker='.', label=labelDict[i] + (" (%.3f)"%auc))
    # axis labels
    pyplot.xlabel('False Positive Rate')
    pyplot.ylabel('True Positive Rate')
    # show the legend
    pyplot.legend()
    # show the plot
    # pyplot.show()
    filename = os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch) + '_' + validationSet + '_roc_'+labelDict[i]+'.png')
    #print("Generando " + filename)
    pyplot.savefig(filename)

    pyplot.clf()

  # Este une los tres gráficos en uno
  middleLinePlotted = False
  for i in labels:
    ns_fpr, ns_tpr, _ = roc_curve(testY, ns_probs[:, i], pos_label = i) 
    lr_fpr, lr_tpr, thresholds = roc_curve(testY, predY[:, i], pos_label = i)

    # plot the roc curve for the model
    if not middleLinePlotted:
      pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='')
      middleLinePlotted = True
    auc = lr_auc[i]
    pyplot.plot(lr_fpr, lr_tpr, marker='.', label=labelDict[i]+ (" (%.3f)"%auc))
    # axis labels
    pyplot.xlabel('False Positive Rate')
    pyplot.ylabel('True Positive Rate')
    # show the legend
    pyplot.legend()
    # show the plot
    # pyplot.show()
    filename = os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch) + '_' + validationSet + '_roc.png')
    #print("Generando " + filename)
    pyplot.savefig(filename)

    f = open(os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch)  + '_' + validationSet + '_' + labelDict[i] + '_roc_stats.txt'), "w")
    f.write("FPR: " + str(lr_fpr) + "\n")
    f.write("TPR: " + str(lr_tpr) + "\n")
    f.write("Thresholds: " + str(thresholds) + "\n")
    f.close()


  pyplot.clf()

  return lr_auc

# Source: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/
def calculateAUCROCs2Classes(testY, predY, experimentOutputFolder, experimentName, num_classes, validationSet = "valFleni", labels = [1,2],   labelDict = {
    0: "CN",
    1: "AD",
    2: "MCI"
  }, epoch = 0, executionNumber = 0):

  if num_classes == 3:
      labels = [ 0, 1 ]          # Seteamos solo 2 labels: 0 (CN/MCI) y 1 (AD)
      testY[ testY == 2] = 0.0   # mergeamos las clases 2 y 0

      # Esto es necesario porque hay algún tipo de problema con roc_auc_score que hace que
      # de un valor de curva mas alto del graficado

      # Mergeamos las predicciones
      predY[:, 0] = predY[:, 0] + predY[:, 2]
      predY = predY[:, 0:2]


  ns_probs = np.empty( shape = predY.shape[0] )
  ns_probs[:] = 0.5  
  
  # calculate scores
  ns_auc = roc_auc_score(testY, ns_probs, labels= labels, average = None)
  lr_auc = roc_auc_score(testY, predY[:, 1], labels= labels, average = None)     # AD

  # print("AUC: ", lr_auc)

  # Un solo gráfico, para AD
  ns_fpr, ns_tpr, _ = roc_curve(testY, ns_probs, pos_label = 1) 
  lr_fpr, lr_tpr, thresholds = roc_curve(testY, predY[:, 1], pos_label = 1)

  # Esto tengo que calcularlo así porque, creo, hay un bug/error al ser dos clases pero con labels [1,2]
  # La otra opción era modificar testY para que las labels "2" sean "0"
  #lr_auc = np.trapz(lr_tpr, lr_fpr)
  #print("Numpy AUC: ", lr_auc)
  
  f = open(os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch)  + '_' + validationSet + '_roc_stats.txt'), "w")
  f.write("AUC: " + str(lr_auc) + "\n")
  f.write("FPR: " + str(lr_fpr) + "\n")
  f.write("TPR: " + str(lr_tpr) + "\n")
  f.write("Thresholds: " + str(thresholds) + "\n")
  f.close()

  # print("ROC")
  # print("lr_fpr")
  # print(lr_fpr)
  # print("lr_tpr")
  # print(lr_tpr)

  pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='')
  pyplot.plot(lr_fpr, lr_tpr, marker='.', label="AD (AUC: %.3f)"%lr_auc)
  # axis labels
  pyplot.xlabel('False Positive Rate')
  pyplot.ylabel('True Positive Rate')
  # show the legend
  pyplot.legend()
  # show the plot
  # pyplot.show()
  filename = os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch) + '_' + validationSet + '_roc.png')
  #print("Generando " + filename)
  pyplot.savefig(filename)

  pyplot.clf()

  return lr_auc
