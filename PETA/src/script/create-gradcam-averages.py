# -*- coding: utf-8 -*-
from __future__ import print_function, division
"""MuestraFull3700_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dR7ZG17_Mmr3y3JV0mLf12Bp8U8J27Aa
"""

import sys

try:
    from google.colab import drive
    drive.mount('/content/gdrive')
except ModuleNotFoundError:
    print('Not running on Google')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import cv2
import os
import time
import copy
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms, utils, models, datasets
import torch.nn as nn
import torch.optim as optim
import nibabel as nib
import scipy.ndimage as ndi
from pathlib import Path
from PIL import Image
import io
import json
import random
import sklearn.metrics
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
import json
from torchinfo import summary

# Own
sys.path.append(os.path.abspath('./../src'))
from util import clipped_zoom, printFile, getClassDescription
from transforms import TransformGridImage, ToLabelOutput, ToLabelOutputFleni
from datasets import FleniMyriamDataset, ADNIDataset
from select_criterias import select_criteria_accuracy_aggregate, select_criteria_accuracy_epoch_result, select_criteria_f1AD_aggregate, select_criteria_f1AD_epoch_result
from util import test_model, printClassStats
from config import loadConfig
from train_lib import train_model, set_parameter_requires_grad, initialize_model
from grad_cam import generateGradCAMMask, generateGradCAM

# CAM
from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
#from pytorch_grad_cam.utils.image import show_cam_on_image
from pytorch_grad_cam import GuidedBackpropReLUModel
from pytorch_grad_cam.utils.image import show_cam_on_image, \
    deprocess_image, \
    preprocess_image

# Ignore warnings
import warnings
warnings.filterwarnings("ignore")
print("PyTorch Version: ",torch.__version__)
print("Torchvision Version: ",torchvision.__version__)

#

def printHelp():
    print("Modo de uso: ")
    print("python3 create-gradcam-averages.py <configFile> <weightsFile> [<outputFolder>]")

if len(sys.argv) == 2 and (sys.argv[1] == "--help" or sys.argv[1] == "-h"):
    printHelp()

if len(sys.argv) < 3:
    printHelp()
    sys.exit(1)

if len(sys.argv) == 4:
    outputDir = sys.argv[3]
else:
    print("Using default dir as current dir")
    outputDir = './'
    
"""# Configuración"""


configFile = sys.argv[1]
weights = sys.argv[2]


config = loadConfig(configFile)

imagesFolder = config['imagesFolder']
fleniImagesFolder = config['fleniImagesFolder']
trainDatasetCSV = config['trainDatasetCSV']
valDatasetCSV = config['valDatasetCSV']
fleniValDatasetCSV = config['fleniValDatasetCSV']
experimentName = config['experimentName']
experimentOutputFolder = config['experimentOutputFolder']
experimentDescription = config['experimentDescription']
executions = config['executions']
model_name = config['model_name'] # Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]
num_classes = config['num_classes'] # Number of classes in the dataset
batch_size = config['batch_size'] # Batch size for training (change depending on how much memory you have)
dl_num_workers = config['dl_num_workers']
num_epochs = config['num_epochs'] # Number of epochs to train for
# Flag for feature extracting. When False, we finetune the whole model,
#   when True we only update the reshaped layer params
feature_extract = config['feature_extract']
usePretrained = config['usePretrained']
auxEnabled = config['auxEnabled'] # Habilita la salida auxiliar
learningRate = config['learningRate']
dropoutRate = config['dropoutRate']
trainMean = config['trainMean']
trainStd = config['trainStd']
deviceName = config['deviceName']
dataAugmentation = config['dataAugmentation']
selectCriteria = config['selectCriteria']
validationCacheSize = config['validationCacheSize']
trainCacheSize = config['trainCacheSize']
calculateAUCROC = config['calculateAUCROC']
debug = config['debug']
doTrain = config['doTrain']
selectCriteriaAbbrv = config['selectCriteriaAbbrv']
trainElements = config['trainElements']
truthLabel = config['truthLabel']

# Útil para procesar solo un subconjunto de epochs
startEpoch = 0
endEpoch = num_epochs
if "eval" in config and "startEpoch" in config["eval"]:
    startEpoch = config["eval"]["startEpoch"]
if "eval" in config and "endEpoch" in config["eval"]:
    endEpoch = config["eval"]["endEpoch"]

processStatsADNI = True
processStatsFleni = True
if "eval" in config:
    evalConfig = config["eval"]
    if "processStatsADNI" in evalConfig:
        processStatsADNI = evalConfig["processStatsADNI"]
    if "processStatsFleni" in evalConfig:
        processStatsFleni = evalConfig["processStatsFleni"]


if num_classes == 3:
  crossEntrophyWeigths = torch.tensor(trainElements) # Órden: CN, AD, MCI
else:
  crossEntrophyWeigths = torch.tensor([trainElements[0] + trainElements[2], trainElements[1]]) # Órden: CN/MCI, AD

#trainMean = 0.1716601789041244 #preproc3, < 0s eliminados
#trainStd = 0.3936839672084841 #preproc3
#trainMean = 0.1534203209139499  #preproc4, sin eliminar < 0s
#trainStd =  0.4048895150096513   #preproc4
normalization = {
  #"trainMeans": [0.485, 0.456, 0.406], # ImageNet
  #"trainStds": [0.229, 0.224, 0.225].  # ImageNet
  "trainMeans": [trainMean, trainMean, trainMean],
  "trainStds": [trainStd, trainStd, trainStd]
}

# Data augmentation
# dataAugmentation = {
#     "angle": 8,
#     "shiftX": 10,
#     "shiftY": 10,
#     "zoom": 0.1,
#     "shear": np.pi / 16,
# }
# dataAugmentation = {}

# End Config

"""# Utilidades"""

selectCriteriaAbbrv = selectCriteriaAbbrv[selectCriteria]


# importar el set de validación y recorrerlo 1 a 1 (por qué solo el set de validación?)

# Initialize the model for this run
model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, dropoutRate, auxEnabled, use_pretrained=usePretrained)

# Print the model we just instantiated
print(model_ft)

# Data augmentation and normalization for training
# Just normalization for validation

# Data augmentation and normalization for training
# Just normalization for validation

valGridArgs = {}

data_transforms = {
    'val': transforms.Compose([
        TransformGridImage(),
        transforms.ToTensor(),
        transforms.Normalize(normalization["trainMeans"], normalization["trainStds"])
    ]),
}

print("Initializing Datasets and Dataloaders...")

# Create training and validation datasets

image_datasets = {
    'val': ADNIDataset('valDL', valDatasetCSV, imagesFolder, transform = data_transforms['val'], target_transform =ToLabelOutput(num_classes), cacheSize = validationCacheSize, truthLabel = truthLabel ),
    'valFleni': FleniMyriamDataset('valFleniDL', fleniValDatasetCSV, fleniImagesFolder, transform = data_transforms['val'], target_transform =ToLabelOutputFleni(num_classes), cacheSize = validationCacheSize ),
}

# Create training and validation dataloaders
# , 'valFleni'
dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=False, num_workers=0) for x in ['val', 'valFleni']}

# Detect if we have a GPU available
device = torch.device(deviceName if torch.cuda.is_available() else "cpu")

# Eval

model_state_dict = torch.load(weights, map_location=device)
model_ft.load_state_dict(model_state_dict)

executionNumber = 0


# para cada capa:

# - para cada input, llamar a generateGradCAMMask

# - guardar en memoria todas las máscaras en un array

# - hacer el promedio 

# - guardar el promedio en un archivo
layers = ['Mixed_6a', 'Mixed_6b', 'Mixed_6c', 'Mixed_6d', 'Mixed_6e']

cams = {}

for layer in layers:
    cams[layer] = np.zeros((num_classes, 512,512)) # un promedio por cada clase

numPredictedClasses = np.zeros((num_classes))

n = len(image_datasets['val'])
print(n)

for param in model_ft.parameters():
    param.requires_grad = True

model_ft.eval()

referenceImage = None

processedImages = 0
for inputs, labels in dataloaders_dict['val']:
    inputs = inputs.to(device)
    outputs = model_ft(inputs)
    _, preds = torch.max(outputs, 1)

    if referenceImage is None:
        referenceImage = inputs[0].clone().detach().cpu().numpy()
        # Normalization [0,1], required by show_cam_on_image
        referenceImage = (referenceImage - np.min(referenceImage))/np.ptp(referenceImage)
        referenceImage = np.transpose( referenceImage, (1,2,0)) # Channels should be at the end
        
    for i in range(0, len(preds)):
        pred = preds[i]
        numPredictedClasses[pred] += 1

    for layer in layers:
        target_layers = list(map( lambda layerName: getattr(model_ft, layerName), [layer]))
        grayscaleCAM = generateGradCAMMask(model_ft, inputs, target_layers)

        print("grayscaleCAM")
        print(np.min(grayscaleCAM), np.max(grayscaleCAM))

        for i in range(0, len(preds)):
            pred = preds[i]
            cams[layer][pred] += grayscaleCAM[i]

    torch.cuda.empty_cache()

    processedImages += batch_size

    print(f"Progress {(processedImages * 100 / n)} %")

print(numPredictedClasses)

print("Ref image")
print(np.min(referenceImage), np.max(referenceImage))

for i in range(0, num_classes):
    print("Class " + str(i))
    print("Len: " + str(numPredictedClasses[i]))
    print(np.min(cams[layer][i]), np.max(cams[layer][i])) 
    for layer in layers:
        grayscale_cam = cams[layer][i] / numPredictedClasses[i]

        print(grayscale_cam.shape)

        # Grayscale, without reference
        rgbImg = np.zeros((512,512,3))

        rgbImg[:, :, 0] = grayscale_cam
        rgbImg[:, :, 1] = grayscale_cam
        rgbImg[:, :, 2] = grayscale_cam

        PIL_image = Image.fromarray((rgbImg*255).astype('uint8'), 'RGB')

        PIL_image.save(os.path.join(outputDir, f'average_{layer}_{i}.png'))

        # In colors, with reference
        rgbImg = referenceImage.copy()
        cam_image = show_cam_on_image(rgbImg, grayscale_cam, use_rgb=True)
        cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(os.path.join(outputDir, f'average_rgb_reference_{layer}_{i}.png'), cam_image)

        # In colors, without reference
        rgbImg = np.zeros((512,512,3))
        cam_image = show_cam_on_image(rgbImg, grayscale_cam, use_rgb=True)
        cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(os.path.join(outputDir, f'average_rgb_{layer}_{i}.png'), cam_image)
