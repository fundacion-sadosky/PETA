{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5365601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import dicom2nifti\n",
    "import nibabel as nib\n",
    "import nilearn as nil\n",
    "import scipy.ndimage as ndi\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "21fde5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/8598673/how-to-save-a-pylab-figure-into-in-memory-file-which-can-be-read-into-pil-image\n",
    "def fig2img(fig):\n",
    "    \"\"\"Convert a Matplotlib figure to a PIL Image and return it\"\"\"\n",
    "    import io\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, dpi = 64) # Requerido para que la imagen sea 512x512\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "503264b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transformGridImage(sample):\n",
    "    brain_vol_data = sample.get_fdata()\n",
    "    fig_rows = 4\n",
    "    fig_cols = 4\n",
    "    n_subplots = fig_rows * fig_cols\n",
    "    n_slice = brain_vol_data.shape[2]\n",
    "    step_size = n_slice // n_subplots\n",
    "    plot_range = n_subplots * step_size\n",
    "    start_stop = int((n_slice - plot_range) / 2)\n",
    "\n",
    "    fig, axs = plt.subplots(fig_rows, fig_cols, figsize=[10, 10], facecolor='black', dpi=64)\n",
    "    fig.set_size_inches(8, 8)\n",
    "    fig.set_dpi(64)\n",
    "    \n",
    "    for idx, img in enumerate(range(start_stop, plot_range, step_size)):\n",
    "        axs.flat[idx].imshow(ndi.rotate(brain_vol_data[:, :, img], 90), cmap='gray')\n",
    "        axs.flat[idx].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "\n",
    "    image = fig2img(fig)\n",
    "    # plt.savefig('filename.png', dpi=64) # guarda en 512x512\n",
    "    # plt.show()\n",
    "\n",
    "    # image.show()\n",
    "\n",
    "    plt.close(fig) # Para que no muestre la imÃ¡gen\n",
    "    \n",
    "    # return sample\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e78aa5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreateGrid transform\n",
    "class CreateGrid(object):\n",
    "    \"\"\"Creates a grid from the image\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        True\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return transformGridImage(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "68efee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToLabelOutput(object):\n",
    "    def __call__(self, label):\n",
    "        if label == \"CN\":\n",
    "            return torch.tensor([1, 0, 0])\n",
    "        elif label == \"AD\":\n",
    "            return torch.tensor([0, 1, 0])\n",
    "        else:\n",
    "            return torch.tensor([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "463e18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADNIDataset(Dataset):\n",
    "    \"\"\"ADNI dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None, target_transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        studyID = self.landmarks_frame.iloc[idx, 0]\n",
    "        subjectID = self.landmarks_frame.iloc[idx, 1]\n",
    "        processFormat = self.landmarks_frame.iloc[idx, 7]\n",
    "        date = self.landmarks_frame.iloc[idx, 9]\n",
    "        diagnosis = self.landmarks_frame.iloc[idx, 2]\n",
    "        \n",
    "        filename = None\n",
    "        \n",
    "        rglob = str(studyID)+'/'+'*.nii'\n",
    "        print(rglob)\n",
    "        samples = 0\n",
    "        for path in Path('ADNI-Full-PostProc').rglob(rglob):\n",
    "            filename = str(path)\n",
    "            samples =+ 1\n",
    "            \n",
    "        if samples > 1:\n",
    "            raise \"Mas de un sample. Error\"\n",
    "\n",
    "        if not filename:\n",
    "            raise \"Not found: \" + filename\n",
    "            \n",
    "        brain_vol = nib.load(filename)\n",
    "    \n",
    "        # sample = {'image': brain_vol, 'diagnosis': diagnosis}\n",
    "\n",
    "        image = brain_vol\n",
    "        label = diagnosis\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e25405d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2219"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adniDataset = ADNIDataset('ADNI-Full-PostProc/ADNI-FULL-PostProc_11_29_2022_UniformResolution.csv', '', transform = CreateGrid())\n",
    "len(adniDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4d423e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I59400/*.nii\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AD'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = adniDataset[518]\n",
    "# brain_vol_data = sample['image'].get_fdata()\n",
    "# diagnosis = sample['diagnosis']\n",
    "image.show()\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75dfb90",
   "metadata": {},
   "source": [
    "# Con dos transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6aa3dca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2219"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adniDataset = ADNIDataset('ADNI-Full-PostProc/ADNI-FULL-PostProc_11_29_2022_UniformResolution.csv', '', transform = transforms.Compose([CreateGrid(), transforms.ToTensor()]), target_transform =ToLabelOutput() )\n",
    "len(adniDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "431daf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I59400/*.nii\n"
     ]
    }
   ],
   "source": [
    "image, label = adniDataset[518]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "21aae804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "28eb7475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b552888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
