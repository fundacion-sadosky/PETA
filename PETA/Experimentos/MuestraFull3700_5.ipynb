{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xhN8vz1PQ8Wv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679327166371,"user_tz":180,"elapsed":2538,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}},"outputId":"0ea0e5ad-b3d9-4ad6-99bf-c285beb31a4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","except ModuleNotFoundError:\n","    print('Not running on Google')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpyelL4yXvC0"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"ChL61Twesn0o"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRvvbIrxP5Hd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679327166372,"user_tz":180,"elapsed":6,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}},"outputId":"ba1cf7f0-e376-4b01-a0b9-d840714d73f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch Version:  1.13.1+cu116\n","Torchvision Version:  0.14.1+cu116\n"]}],"source":["from __future__ import print_function, division\n","import os\n","import time\n","import copy\n","import torch\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from torchvision import transforms, utils, models, datasets\n","import torch.nn as nn\n","import torch.optim as optim\n","import nibabel as nib\n","import scipy.ndimage as ndi\n","from pathlib import Path\n","from PIL import Image\n","import io\n","import json\n","import random\n","import sklearn.metrics\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","from matplotlib import pyplot\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"]},{"cell_type":"markdown","metadata":{"id":"INZ-JH53dh-Z"},"source":["# Configuración"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HErqZ9GNCDq6"},"outputs":[],"source":["imagesFolder = '/content/gdrive/MyDrive/Tesis/Imagenes/ADNI-MUESTRA-FULL-stripped-preprocessed3'\n","fleniImagesFolder = '/content/gdrive/MyDrive/Tesis/Imagenes/fleni-stripped-preprocessed3'\n","trainDatasetCSV = '/content/gdrive/MyDrive/Tesis/Imagenes/ADNI-MUESTRA-930/MUESTRA_train.csv'\n","valDatasetCSV =   '/content/gdrive/MyDrive/Tesis/Imagenes/ADNI-MUESTRA-930/MUESTRA_val.csv'\n","fleniValDatasetCSV =   '/content/gdrive/MyDrive/Tesis/Imagenes/fleni-stripped-preprocessed/match-curated.csv'\n","experimentName = 'MuestraFull3700_5'\n","experimentOutputFolder = '/content/gdrive/MyDrive/Tesis/Experimentos/muestraFull3700_5'\n","experimentDescription = 'Colapsar clases. Prueba 25 epochs con 2 clases.'\n","executions = 1"]},{"cell_type":"markdown","metadata":{"id":"0tgyNu3NVCe9"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYNyT00wpzwU"},"outputs":[],"source":["# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n","model_name = \"inception\"\n","\n","# Number of classes in the dataset\n","num_classes = 2\n","\n","# Batch size for training (change depending on how much memory you have)\n","batch_size = 32\n","\n","dl_num_workers = 4\n","\n","# Number of epochs to train for\n","num_epochs = 25\n","\n","# Flag for feature extracting. When False, we finetune the whole model,\n","#   when True we only update the reshaped layer params\n","feature_extract = False\n","\n","usePretrained = True\n","\n","# Habilita la salida auxiliar\n","auxEnabled = True\n","\n","learningRate = 0.0001\n","dropoutRate = 0.6\n","if num_classes == 3:\n","  crossEntrophyWeigths = torch.tensor([759.0,444.0,1717.0]) # Órden: CN, AD, MCI\n","else:\n","  crossEntrophyWeigths = torch.tensor([759.0 + 1717.0,444.0]) # Órden: CN/MCI, AD\n","\n","trainMean = 0.1716601789041244 #preproc3, < 0s eliminados\n","trainStd = 0.3936839672084841 #preproc3\n","#trainMean = 0.1534203209139499  #preproc4, sin eliminar < 0s\n","#trainStd =  0.4048895150096513   #preproc4\n","normalization = {\n","  #\"trainMeans\": [0.485, 0.456, 0.406], # ImageNet\n","  #\"trainStds\": [0.229, 0.224, 0.225].  # ImageNet\n","  \"trainMeans\": [trainMean, trainMean, trainMean],\n","  \"trainStds\": [trainStd, trainStd, trainStd]\n","}\n","\n","deviceName = 'cuda:0'\n","\n","# Data augmentation\n","dataAugmentation = {\n","    \"angle\": 8,\n","    \"shiftX\": 10,\n","    \"shiftY\": 10,\n","    \"zoom\": 0.1,\n","    \"shear\": np.pi / 16,\n","}\n","# dataAugmentation = {}\n","\n","selectCriteria = \"f1AD\"\n","\n","validationCacheSize = 0\n","trainCacheSize = 0\n","\n","calculateAUCROC = True # For this is necessary to save all epochs?\n","\n","# Debug\n","debug = False\n","doTrain = True # True unless we want to skip training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYl8O7nNtSeo"},"outputs":[],"source":["if not os.path.exists(experimentOutputFolder):\n","  print(\"Creando carpeta \" + experimentOutputFolder)\n","  os.mkdir(experimentOutputFolder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2ZFcEXjwezm"},"outputs":[],"source":["f = open(os.path.join(experimentOutputFolder, experimentName + \"_params.txt\"), \"w\")\n","f.write(\"batch_size: \" + str(batch_size) + \"\\n\")\n","f.write(\"dl_num_workers: \" + str(dl_num_workers) + \"\\n\")\n","f.write(\"epochs: \" + str(num_epochs) + \"\\n\")\n","f.write(\"feature_extract: \" + str(feature_extract) + \"\\n\")\n","f.write(\"usePretrained: \" + str(usePretrained) + \"\\n\")\n","f.write(\"auxEnabled: \" + str(auxEnabled) + \"\\n\")\n","f.write(\"learningRate: \" + str(learningRate) + \"\\n\")\n","f.write(\"dropoutRate: \" + str(dropoutRate) + \"\\n\")\n","f.write(\"cross entrophy weights: \" + str(crossEntrophyWeigths) + \"\\n\")\n","f.write(\"dataAugmentation: \" + str(json.dumps(dataAugmentation)) + \"\\n\")\n","f.write(\"selectCriteria: \" + str(selectCriteria) + \"\\n\")\n","f.write(\"executions: \" + str(executions) + \"\\n\")\n","f.write(\"normalization: \" + str(json.dumps(normalization)) + \"\\n\")\n","f.write(\"deviceName: \" + str(deviceName) + \"\\n\")\n","f.write(\"validationCacheSize: \" + str(validationCacheSize) + \"\\n\")\n","f.write(\"trainCacheSize: \" + str(trainCacheSize) + \"\\n\")\n","f.write(\"calculateAURROC: \" + str(calculateAUCROC) + \"\\n\")\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJYNgYD9AC4X"},"outputs":[],"source":["f = open(os.path.join(experimentOutputFolder, experimentName + \"_descripcion.txt\"), \"w\")\n","f.write(experimentDescription)\n","f.close()"]},{"cell_type":"markdown","metadata":{"id":"ICvsarqhdkaI"},"source":["# Utilidades"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoU0NRC1-jnh"},"outputs":[],"source":["selectCriteriaAbbrv = {\n","    \"accuracy\": \"acc\",\n","    \"f1AD\": \"f1AD\",\n","}[selectCriteria]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbR_0LKedYLw"},"outputs":[],"source":["def logDebug(str):\n","    if debug:\n","        print(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-Z1h8wrQaiB"},"outputs":[],"source":["def clipped_zoom(img, zoom_factor, **kwargs):\n","\n","    h, w = img.shape[:2]\n","\n","    # For multichannel images we don't want to apply the zoom factor to the RGB\n","    # dimension, so instead we create a tuple of zoom factors, one per array\n","    # dimension, with 1's for any trailing dimensions after the width and height.\n","    zoom_tuple = (zoom_factor,) * 2 + (1,) * (img.ndim - 2)\n","\n","    # Zooming out\n","    if zoom_factor < 1:\n","\n","        # Bounding box of the zoomed-out image within the output array\n","        zh = int(np.round(h * zoom_factor))\n","        zw = int(np.round(w * zoom_factor))\n","        top = (h - zh) // 2\n","        left = (w - zw) // 2\n","\n","        # Zero-padding\n","        out = np.zeros_like(img)\n","        out[top:top+zh, left:left+zw] = ndi.zoom(img, zoom_tuple, **kwargs)\n","\n","    # Zooming in\n","    elif zoom_factor > 1:\n","\n","        # Bounding box of the zoomed-in region within the input array\n","        zh = int(np.round(h / zoom_factor))\n","        zw = int(np.round(w / zoom_factor))\n","        top = (h - zh) // 2\n","        left = (w - zw) // 2\n","\n","        out = ndi.zoom(img[top:top+zh, left:left+zw], zoom_tuple, **kwargs)\n","\n","        # `out` might still be slightly larger than `img` due to rounding, so\n","        # trim off any extra pixels at the edges\n","        trim_top = ((out.shape[0] - h) // 2)\n","        trim_left = ((out.shape[1] - w) // 2)\n","        out = out[trim_top:trim_top+h, trim_left:trim_left+w]\n","\n","    # If zoom_factor == 1, just return the input array\n","    else:\n","        out = img\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eacnqPB5rkv5"},"outputs":[],"source":["class LoadImage(object):\n","    \"\"\"Loads an image\n","    \"\"\"\n","\n","    def __init__(self, imagesFolder):\n","      self.imagesFolder = imagesFolder\n","\n","    def __call__(self, studyID):\n","      imageFile = os.path.join(self.imagesFolder, studyID, \"resampled-normalized.nii\")\n","      metadataFile = os.path.join(self.imagesFolder, studyID, \"metadata.json\")\n","      f = open(metadataFile, \"r\")\n","      metadata = json.load(f)\n","      f.close()\n","\n","      return nib.load(imageFile), metadata"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVI5nxyh6Jfa"},"outputs":[],"source":["class TransformGridImage():\n","    def __init__(self, angle = None, zoom = None, shiftX = None, shiftY = None, shear = None):\n","      self.angle = angle\n","      self.zoom = zoom\n","      self.shiftX = shiftX\n","      self.shiftY = shiftY\n","      self.shear = shear\n","      self.sliceWidth = 128\n","      self.sliceHeight = 128\n","      self.filters = []\n","      if (self.zoom):\n","        self.filters.append(\"zoom\")\n","      if (self.angle):\n","        self.filters.append(\"angle\")\n","      if self.shiftX:\n","        self.filters.append(\"shiftX\")\n","      if self.shiftY:\n","        self.filters.append(\"shiftY\")\n","      if self.shear:\n","        self.filters.append(\"shear\")\n","\n","    def __call__(self, studyData):\n","      sample = studyData[0]\n","      metadata = studyData[1]\n","\n","      brain_vol_data = sample.get_fdata()\n","      fig_rows = 4\n","      fig_cols = 4\n","      n_subplots = fig_rows * fig_cols\n","\n","      deleteIndices = metadata[\"deleteIndices\"]\n","    \n","      brain_vol_data = np.delete(brain_vol_data, deleteIndices, axis=2)\n","    \n","      n_slice = brain_vol_data.shape[2]\n","\n","      step_size = n_slice / n_subplots\n","\n","      slice_indices = np.arange(0, n_slice, step = step_size)\n","\n","      channels = 3\n","      grid = np.empty( shape = (fig_rows * 128, fig_cols * 128, channels), dtype=np.float32)\n","\n","      angle = 0.0\n","      zoom = None\n","      shiftX = None\n","      shiftY = None\n","      shear = None\n","\n","      filter = None\n","      if len(self.filters) > 0:\n","        filter = random.choice(self.filters)\n","    \n","      if filter == \"angle\":\n","        angle = random.uniform(-self.angle, self.angle)\n","      elif filter == \"zoom\":\n","        zoom = 1.0 + random.uniform(-self.zoom, self.zoom)\n","      elif filter == \"shiftX\":\n","        shiftX = round(128 * random.uniform(-self.shiftX, self.shiftX) / 100.0)\n","      elif self.shiftY == \"shiftY\":\n","        shiftY = round(128 * random.uniform(-self.shiftY, self.shiftY) / 100.0)\n","      elif filter == \"shear\":\n","        shear = random.uniform(-self.shear, self.shear)\n","        \n","      slice_index = 0\n","      for i in range(0, fig_rows):\n","        for j in range(0, fig_cols):\n","            slice_index  = slice_indices[i * fig_rows + j]\n","            processedImage = ndi.rotate(brain_vol_data[:, :, round(slice_index)], 90.0 + angle, mode='nearest', reshape = False)\n","\n","            if zoom != None:\n","                processedImage = clipped_zoom(processedImage, zoom, mode = 'nearest')\n","\n","            if shiftX != None:\n","                processedImage = ndi.shift(processedImage, [0.0, shiftX], mode = 'nearest')\n","\n","            if shiftY != None:\n","                processedImage = ndi.shift(processedImage, [shiftY, 0.0], mode = 'nearest')\n","\n","            if shear != None:\n","                # shear debe estar en radianes\n","                # https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/affine_transformations.py#L348\n","                transform = np.array([[1, -np.sin(shear), 0],\n","                                 [0, np.cos(shear), 0],\n","                                 [0, 0, 1]])\n","                processedImage = ndi.affine_transform(processedImage,\n","                    transform,\n","                    #offset=(0, -self.sliceHeight//2, 0),\n","                    output_shape=(self.sliceWidth, self.sliceHeight))\n","\n","            rowStart = i * 128\n","            rowEnd = (i + 1) * 128\n","            colStart = j * 128\n","            colEnd = (j+1)*128\n","            \n","            # 3 channels\n","            for c in range(0, channels):\n","              grid[rowStart:rowEnd, colStart:colEnd, c] = processedImage.copy()\n","\n","            slice_index += 1\n","\n","      return grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UmM3wjZ0Qfa4"},"outputs":[],"source":["class ToLabelOutput(object):\n","    def __init__(self, numClasses = 3):\n","        self.numClasses = numClasses\n","\n","    def __call__(self, label):\n","        if self.numClasses == 3:\n","          if label == \"CN\":\n","            return 0\n","          elif label == \"AD\":\n","            return 1\n","          else:\n","            return 2 # MCI, LMCI, EMC\n","        else:\n","          if label == \"AD\":\n","            return 1\n","          else:\n","            return 0 # CN/MCI collapsed class"]},{"cell_type":"code","source":["class ToLabelOutputFleni(object):\n","    def __init__(self, numClasses = 3):\n","        self.numClasses = numClasses\n","\n","    def __call__(self, label):\n","      if int(label) == 1:\n","        return 1 # AD\n","      elif int(label) == 0:\n","        if self.numClasses == 3: \n","          return 2 # MCI\n","        else:\n","          return 0 # CN/MCI collapsed class\n","      else:\n","        raise Exception(\"Wrong Fleni label: \" + str(label))"],"metadata":{"id":"s1l38azEVGYq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ve2taCO5YevV"},"outputs":[],"source":["class FleniMyriamDataset(Dataset):\n","    \"\"\"Fleni Myriam dataset.\"\"\"\n","\n","    def __init__(self, name, csv_file, root_dir, transform=None, target_transform = None, \n","                 cacheSize = 200):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.name = name\n","        self.csv = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        # item_cache directamente almacena los items procesados\n","        self.cacheSize = cacheSize\n","        self.item_cache = [None] * cacheSize\n","        self.cachedItems = 0\n","\n","    def __len__(self):\n","      return int(len(self.csv))\n","\n","    def storeInCache(self, idx, image, label, metadata):\n","        if self.cacheSize > 0 and self.item_cache[idx % self.cacheSize] == None:  \n","            logDebug(self.name + \"] Storing item in cache: \" + str(idx))\n","            self.cachedItems += 1\n","            # Storing item in cache\n","            self.item_cache[idx % self.cacheSize] = {\n","                \"id\": idx,\n","                \"label\": label,\n","                \"image\": image,\n","                \"metadata\": metadata\n","            }\n","            logDebug(self.name + \"] Cached items: \" + str(self.cachedItems))\n","\n","    def itemInCache(self, idx):\n","        if self.cacheSize > 0 and self.item_cache[idx % self.cacheSize] != None and self.item_cache[idx % self.cacheSize][\"id\"] == idx:\n","            return self.item_cache[idx % self.cacheSize]\n","        else:\n","            return None\n","\n","    def loadImage(self, studyID):\n","      imageFile = os.path.join(self.root_dir, studyID, \"resampled-normalized.nii\")\n","      metadataFile = os.path.join(self.root_dir, studyID, \"metadata.json\")\n","      f = open(metadataFile, \"r\")\n","      metadata = json.load(f)\n","      f.close()\n","\n","      return nib.load(imageFile), metadata\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        if self.itemInCache(idx):\n","            item = self.itemInCache(idx)\n","            image = item[\"image\"]\n","            metadata = item[\"metadata\"]\n","            label = item[\"label\"]\n","        else:\n","          studyID = self.csv.iloc[idx]['pet_id']\n","          #subjectID = self.csv.iloc[idx, 1]\n","          #processFormat = self.csv.iloc[idx, 7]\n","          #date = self.csv.iloc[idx, 9]\n","          diag = self.csv.iloc[idx]['diag']\n","          label = diag\n","\n","          image, metadata = self.loadImage(studyID)\n","\n","          self.storeInCache(idx, image, label, metadata)\n","        \n","        if self.transform:\n","            image = self.transform([image, metadata])\n","            \n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmEN7Kfc31ge"},"outputs":[],"source":["class ADNIDataset(Dataset):\n","    \"\"\"ADNI dataset.\"\"\"\n","\n","    def __init__(self, name, csv_file, root_dir, transform=None, target_transform = None, \n","                 cacheSize = 200, indexOffset = 0):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.name = name\n","        self.csv = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        # item_cache directamente almacena los items procesados\n","        self.cacheSize = cacheSize\n","        self.item_cache = [None] * cacheSize\n","        self.cachedItems = 0\n","        self.indexOffset = indexOffset\n","\n","    def __len__(self):\n","      return int(len(self.csv))\n","\n","    def storeInCache(self, idx, image, label, metadata):\n","        if self.cacheSize > 0 and self.item_cache[idx % self.cacheSize] == None:  \n","            logDebug(self.name + \"] Storing item in cache: \" + str(idx))\n","            self.cachedItems += 1\n","            # Storing item in cache\n","            self.item_cache[idx % self.cacheSize] = {\n","                \"id\": idx,\n","                \"label\": label,\n","                \"image\": image,\n","                \"metadata\": metadata\n","            }\n","            logDebug(self.name + \"] Cached items: \" + str(self.cachedItems))\n","\n","    def itemInCache(self, idx):\n","        if self.cacheSize > 0 and self.item_cache[idx % self.cacheSize] != None and self.item_cache[idx % self.cacheSize][\"id\"] == idx:\n","            return self.item_cache[idx % self.cacheSize]\n","        else:\n","            return None\n","\n","    def loadImage(self, studyID):\n","      imageFile = os.path.join(self.root_dir, studyID, \"resampled-normalized.nii\")\n","      metadataFile = os.path.join(self.root_dir, studyID, \"metadata.json\")\n","      f = open(metadataFile, \"r\")\n","      metadata = json.load(f)\n","      f.close()\n","\n","      return nib.load(imageFile), metadata\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        if self.itemInCache(idx):\n","            item = self.itemInCache(idx)\n","            image = item[\"image\"]\n","            metadata = item[\"metadata\"]\n","            label = item[\"label\"]\n","        else:\n","          # +1 por el índice que guarda Pandas\n","          studyID = self.csv.iloc[idx, 0 + self.indexOffset]\n","          label = self.csv.iloc[idx, 2 + self.indexOffset]\n","\n","          image, metadata = self.loadImage(studyID)\n","          self.storeInCache(idx, image, label, metadata)\n","        \n","\n","        if self.transform:\n","            image = self.transform([image, metadata])\n","            \n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMPKNDmqyALS"},"outputs":[],"source":["def printFile(text, file):\n","  print(text)\n","  if file != None:\n","      file.write(text + \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"zW1D_vQnpzDk"},"source":["# Modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-C4YSewwCFi"},"outputs":[],"source":["def select_criteria_accuracy_aggregate(running_corrects, outputs, labels):\n","  _, preds = torch.max(outputs, 1)\n","  return running_corrects + torch.sum(preds == labels.data)\n","\n","def select_criteria_accuracy_epoch_result(running_corrects, dataset):\n","  return running_corrects.double() / len(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSe98FPywywx"},"outputs":[],"source":["def select_criteria_f1AD_aggregate(aggregate, outputs, labels):\n","  _, preds = torch.max(outputs, 1)\n","  if aggregate == None: # init\n","    aggregate = {\n","        \"preds\": torch.clone(preds),\n","        \"labels\": torch.clone(labels.data),\n","    }\n","    return aggregate \n","  else:\n","    aggregate[\"preds\"] = torch.cat((aggregate[\"preds\"], torch.clone(preds)))\n","    aggregate[\"labels\"] = torch.cat((aggregate[\"labels\"], torch.clone(labels.data)))\n","    return aggregate \n","\n","def select_criteria_f1AD_epoch_result(aggregate, dataset):\n","  fn = 0\n","  fp = 0\n","  tp = 0\n","  preds = aggregate[\"preds\"]\n","  labels = aggregate[\"labels\"]\n","  AD = ToLabelOutput(num_classes)(\"AD\")\n","  for i in range(0, len(preds)):\n","    if preds[i] != AD and labels[i] != AD: # solo nos interesa AD\n","      continue\n","    if preds[i] == AD and labels[i] == AD:\n","      tp += 1\n","    if preds[i] == AD and labels[i] != AD:\n","      fp += 1\n","    if preds[i] != AD and labels[i] == AD:\n","      fn += 1\n","\n","  # avoid division by zero\n","  # https://stats.stackexchange.com/questions/8025/what-are-correct-values-for-precision-and-recall-when-the-denominators-equal-0\n","  if tp == 0 and (fp != 0 or fn != 0):\n","    print(\"\\1\")\n","    return 0\n","  elif tp == 0:\n","    print(\"\\2\")\n","    return 1\n","\n","  recall = 1.0 * tp / (tp + fn)\n","  precision = 1.0 * tp / (tp + fp)\n","  return 2.0 * precision * recall / (precision + recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m44_ZbqJvsQ9"},"outputs":[],"source":["def train_model(model, dataloaders, criterion, optimizer, experimentExecutionName, num_epochs=25, is_inception=True, logFile = None, selection_criteria = \"accuracy\", save_all_epochs = False):\n","    f = None\n","    if logFile != None:\n","        f = open(logFile, \"w\")\n","\n","    since = time.time()\n","\n","    train_acc_history = []\n","    val_acc_history = []\n","    train_selection_criteria_history = []\n","    val_selection_criteria_history = []\n","    train_loss_history = []\n","    val_loss_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_selection_criteria = 0.0\n","\n","    # This allows to select different select criteria for selecting the \"best epoch\" one\n","    if selection_criteria == \"accuracy\":\n","      select_criteria_aggregate = select_criteria_accuracy_aggregate\n","      select_criteria_epoch_result = select_criteria_accuracy_epoch_result\n","    elif selection_criteria == \"f1AD\":\n","      select_criteria_aggregate = select_criteria_f1AD_aggregate\n","      select_criteria_epoch_result = select_criteria_f1AD_epoch_result      \n","    else:\n","      raise Exception(\"Invalid selection criteria\")\n","\n","    for epoch in range(num_epochs):\n","        printFile('Epoch {}/{}'.format(epoch, num_epochs - 1), f)\n","        printFile('-' * 10, f)\n","\n","        # Each epoch has a training and validation phase\n","        # , 'valFleni'\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","            selection_criteria_aggregate = None\n","            it = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    if is_inception and phase == 'train':\n","                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n","                        outputs, aux_outputs = model(inputs)\n","                        if auxEnabled: \n","                          loss1 = criterion(outputs, labels)\n","                          loss2 = criterion(aux_outputs, labels)\n","                          loss = loss1 + 0.4*loss2                     \n","                        else:\n","                          loss = criterion(outputs, labels)\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += + torch.sum(preds == labels.data)\n","                # if phase == 'val':\n","                selection_criteria_aggregate = select_criteria_aggregate(selection_criteria_aggregate, outputs, labels.data)\n","\n","                logDebug(\"Iteration \" + str(it))\n","                it += 1\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","            # if phase == 'val':\n","            epoch_selection_criteria = select_criteria_epoch_result(selection_criteria_aggregate, dataloaders[phase].dataset)\n","\n","            printFile('{} Loss: {:.4f} Acc: {:.4f} {}: {:.4f}'.format(phase, epoch_loss, epoch_acc, selection_criteria, epoch_selection_criteria), f)\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_selection_criteria > best_selection_criteria:\n","                best_selection_criteria = epoch_selection_criteria\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","                val_loss_history.append(epoch_loss)\n","                val_selection_criteria_history.append(epoch_selection_criteria)\n","            if phase == 'train':\n","                train_acc_history.append(epoch_acc)\n","                train_loss_history.append(epoch_loss)\n","                train_selection_criteria_history.append(epoch_selection_criteria)\n","\n","                if save_all_epochs:\n","                  fileName = os.path.join(experimentOutputFolder, experimentExecutionName + '_epoch' + str(epoch) + '.pth')\n","                  torch.save(model.state_dict(), fileName)\n","            \n","\n","    time_elapsed = time.time() - since\n","    printFile('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60), f)\n","    printFile('Best val {}: {:4f}'.format(selection_criteria, best_selection_criteria), f)\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    if logFile != None:\n","        f.close()\n","    return model, val_acc_history, val_loss_history, train_acc_history, train_loss_history, val_selection_criteria_history, train_selection_criteria_history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vl-UuMFJv0_S"},"outputs":[],"source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"]},{"cell_type":"markdown","metadata":{"id":"DuMUC0zqwFLw"},"source":["# Initialize and reshape inception"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1679327167043,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"},"user_tz":180},"id":"GqZfuwYwwLw2","outputId":"88f30adf-96ae-4dbe-9e35-a81b08ec94c6","scrolled":false},"outputs":[{"output_type":"stream","name":"stdout","text":["num featurs2048\n","Inception3(\n","  (Conv2d_1a_3x3): BasicConv2d(\n","    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (Conv2d_2a_3x3): BasicConv2d(\n","    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n","    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (Conv2d_2b_3x3): BasicConv2d(\n","    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (Conv2d_3b_1x1): BasicConv2d(\n","    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (Conv2d_4a_3x3): BasicConv2d(\n","    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n","    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (Mixed_5b): InceptionA(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_1): BasicConv2d(\n","      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_2): BasicConv2d(\n","      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3): BasicConv2d(\n","      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_5c): InceptionA(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_1): BasicConv2d(\n","      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_2): BasicConv2d(\n","      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3): BasicConv2d(\n","      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_5d): InceptionA(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_1): BasicConv2d(\n","      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_2): BasicConv2d(\n","      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3): BasicConv2d(\n","      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_6a): InceptionB(\n","    (branch3x3): BasicConv2d(\n","      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3): BasicConv2d(\n","      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_6b): InceptionC(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_1): BasicConv2d(\n","      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_2): BasicConv2d(\n","      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_3): BasicConv2d(\n","      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_1): BasicConv2d(\n","      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_2): BasicConv2d(\n","      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_3): BasicConv2d(\n","      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_4): BasicConv2d(\n","      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_5): BasicConv2d(\n","      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_6c): InceptionC(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_1): BasicConv2d(\n","      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_2): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_3): BasicConv2d(\n","      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_1): BasicConv2d(\n","      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_2): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_3): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_4): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_5): BasicConv2d(\n","      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_6d): InceptionC(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_1): BasicConv2d(\n","      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_2): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_3): BasicConv2d(\n","      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_1): BasicConv2d(\n","      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_2): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_3): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_4): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_5): BasicConv2d(\n","      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_6e): InceptionC(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_2): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_3): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_2): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_3): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_4): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_5): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (AuxLogits): InceptionAux(\n","    (conv0): BasicConv2d(\n","      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv1): BasicConv2d(\n","      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (fc): Sequential(\n","      (0): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (Mixed_7a): InceptionD(\n","    (branch3x3_1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_2): BasicConv2d(\n","      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7x3_1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7x3_2): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7x3_3): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7x3_4): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_7b): InceptionE(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_1): BasicConv2d(\n","      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_2a): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_2b): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3a): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3b): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_7c): InceptionE(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_1): BasicConv2d(\n","      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_2a): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_2b): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3a): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3b): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (dropout): Dropout(p=0.6, inplace=False)\n","  (fc): Sequential(\n","    (0): Linear(in_features=2048, out_features=1024, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=1024, out_features=2, bias=True)\n","  )\n",")\n"]}],"source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"inception\":\n","        \"\"\" Inception v3\n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained, \n","                                       aux_logits = True)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        # num_ftrs = model_ft.AuxLogits.fc.in_features\n","        # model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        print(\"num featurs\" + str(num_ftrs))\n","        # Fuente: https://github.com/bdrad/petdementiapub/blob/master/petdementia_source.py\n","        model_ft.dropout = nn.Dropout(dropoutRate)\n","        model_ft.fc = nn.Sequential(\n","          nn.Linear(num_ftrs,1024),\n","          nn.ReLU(),\n","          nn.Linear(1024,num_classes),\n","        )\n","\n","        if auxEnabled :\n","          model_ft.AuxLogits.fc = nn.Sequential(\n","            nn.Linear(768,num_classes), # elegido arbitrariamentoe\n","          )\n","          \n","        input_size = 512 \n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","\n","    return model_ft, input_size\n","\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=usePretrained)\n","\n","# Print the model we just instantiated\n","print(model_ft)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1679327167890,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"},"user_tz":180},"id":"pfACeWNcwYqY","outputId":"3ccbd08d-6fe5-40fe-ea76-8f654dd2f09c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing Datasets and Dataloaders...\n"]}],"source":["# Data augmentation and normalization for training\n","# Just normalization for validation\n","\n","valGridArgs = {}\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        #LoadImage(imagesFolder),\n","        TransformGridImage(**dataAugmentation),\n","        transforms.ToTensor(),\n","        transforms.Normalize(normalization[\"trainMeans\"], normalization[\"trainStds\"])\n","    ]),\n","    'val': transforms.Compose([\n","        #LoadImage(imagesFolder),\n","        TransformGridImage(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(normalization[\"trainMeans\"], normalization[\"trainStds\"])\n","    ]),\n","}\n","\n","print(\"Initializing Datasets and Dataloaders...\")\n","\n","# Create training and validation datasets\n","\n","image_datasets = {\n","    'train': ADNIDataset('trainDL', trainDatasetCSV, imagesFolder, transform = data_transforms['train'], target_transform =ToLabelOutput(num_classes), cacheSize = trainCacheSize),\n","    'val': ADNIDataset('valDL', valDatasetCSV, imagesFolder, transform = data_transforms['val'], target_transform =ToLabelOutput(num_classes), cacheSize = validationCacheSize ),\n","    'valFleni': FleniMyriamDataset('valFleniDL', fleniValDatasetCSV, fleniImagesFolder, transform = data_transforms['val'], target_transform =ToLabelOutputFleni(num_classes), cacheSize = validationCacheSize ),\n","}\n","\n","# Create training and validation dataloaders\n","# , 'valFleni'\n","dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=dl_num_workers) for x in ['train', 'val', 'valFleni']}\n","\n","# Detect if we have a GPU available\n","device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1679327167890,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"},"user_tz":180},"id":"XciJ190PwerB","outputId":"00408b22-0d47-4f3d-a3d7-00d947c8e88b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Params to learn:\n","\t Conv2d_1a_3x3.conv.weight\n","\t Conv2d_1a_3x3.bn.weight\n","\t Conv2d_1a_3x3.bn.bias\n","\t Conv2d_2a_3x3.conv.weight\n","\t Conv2d_2a_3x3.bn.weight\n","\t Conv2d_2a_3x3.bn.bias\n","\t Conv2d_2b_3x3.conv.weight\n","\t Conv2d_2b_3x3.bn.weight\n","\t Conv2d_2b_3x3.bn.bias\n","\t Conv2d_3b_1x1.conv.weight\n","\t Conv2d_3b_1x1.bn.weight\n","\t Conv2d_3b_1x1.bn.bias\n","\t Conv2d_4a_3x3.conv.weight\n","\t Conv2d_4a_3x3.bn.weight\n","\t Conv2d_4a_3x3.bn.bias\n","\t Mixed_5b.branch1x1.conv.weight\n","\t Mixed_5b.branch1x1.bn.weight\n","\t Mixed_5b.branch1x1.bn.bias\n","\t Mixed_5b.branch5x5_1.conv.weight\n","\t Mixed_5b.branch5x5_1.bn.weight\n","\t Mixed_5b.branch5x5_1.bn.bias\n","\t Mixed_5b.branch5x5_2.conv.weight\n","\t Mixed_5b.branch5x5_2.bn.weight\n","\t Mixed_5b.branch5x5_2.bn.bias\n","\t Mixed_5b.branch3x3dbl_1.conv.weight\n","\t Mixed_5b.branch3x3dbl_1.bn.weight\n","\t Mixed_5b.branch3x3dbl_1.bn.bias\n","\t Mixed_5b.branch3x3dbl_2.conv.weight\n","\t Mixed_5b.branch3x3dbl_2.bn.weight\n","\t Mixed_5b.branch3x3dbl_2.bn.bias\n","\t Mixed_5b.branch3x3dbl_3.conv.weight\n","\t Mixed_5b.branch3x3dbl_3.bn.weight\n","\t Mixed_5b.branch3x3dbl_3.bn.bias\n","\t Mixed_5b.branch_pool.conv.weight\n","\t Mixed_5b.branch_pool.bn.weight\n","\t Mixed_5b.branch_pool.bn.bias\n","\t Mixed_5c.branch1x1.conv.weight\n","\t Mixed_5c.branch1x1.bn.weight\n","\t Mixed_5c.branch1x1.bn.bias\n","\t Mixed_5c.branch5x5_1.conv.weight\n","\t Mixed_5c.branch5x5_1.bn.weight\n","\t Mixed_5c.branch5x5_1.bn.bias\n","\t Mixed_5c.branch5x5_2.conv.weight\n","\t Mixed_5c.branch5x5_2.bn.weight\n","\t Mixed_5c.branch5x5_2.bn.bias\n","\t Mixed_5c.branch3x3dbl_1.conv.weight\n","\t Mixed_5c.branch3x3dbl_1.bn.weight\n","\t Mixed_5c.branch3x3dbl_1.bn.bias\n","\t Mixed_5c.branch3x3dbl_2.conv.weight\n","\t Mixed_5c.branch3x3dbl_2.bn.weight\n","\t Mixed_5c.branch3x3dbl_2.bn.bias\n","\t Mixed_5c.branch3x3dbl_3.conv.weight\n","\t Mixed_5c.branch3x3dbl_3.bn.weight\n","\t Mixed_5c.branch3x3dbl_3.bn.bias\n","\t Mixed_5c.branch_pool.conv.weight\n","\t Mixed_5c.branch_pool.bn.weight\n","\t Mixed_5c.branch_pool.bn.bias\n","\t Mixed_5d.branch1x1.conv.weight\n","\t Mixed_5d.branch1x1.bn.weight\n","\t Mixed_5d.branch1x1.bn.bias\n","\t Mixed_5d.branch5x5_1.conv.weight\n","\t Mixed_5d.branch5x5_1.bn.weight\n","\t Mixed_5d.branch5x5_1.bn.bias\n","\t Mixed_5d.branch5x5_2.conv.weight\n","\t Mixed_5d.branch5x5_2.bn.weight\n","\t Mixed_5d.branch5x5_2.bn.bias\n","\t Mixed_5d.branch3x3dbl_1.conv.weight\n","\t Mixed_5d.branch3x3dbl_1.bn.weight\n","\t Mixed_5d.branch3x3dbl_1.bn.bias\n","\t Mixed_5d.branch3x3dbl_2.conv.weight\n","\t Mixed_5d.branch3x3dbl_2.bn.weight\n","\t Mixed_5d.branch3x3dbl_2.bn.bias\n","\t Mixed_5d.branch3x3dbl_3.conv.weight\n","\t Mixed_5d.branch3x3dbl_3.bn.weight\n","\t Mixed_5d.branch3x3dbl_3.bn.bias\n","\t Mixed_5d.branch_pool.conv.weight\n","\t Mixed_5d.branch_pool.bn.weight\n","\t Mixed_5d.branch_pool.bn.bias\n","\t Mixed_6a.branch3x3.conv.weight\n","\t Mixed_6a.branch3x3.bn.weight\n","\t Mixed_6a.branch3x3.bn.bias\n","\t Mixed_6a.branch3x3dbl_1.conv.weight\n","\t Mixed_6a.branch3x3dbl_1.bn.weight\n","\t Mixed_6a.branch3x3dbl_1.bn.bias\n","\t Mixed_6a.branch3x3dbl_2.conv.weight\n","\t Mixed_6a.branch3x3dbl_2.bn.weight\n","\t Mixed_6a.branch3x3dbl_2.bn.bias\n","\t Mixed_6a.branch3x3dbl_3.conv.weight\n","\t Mixed_6a.branch3x3dbl_3.bn.weight\n","\t Mixed_6a.branch3x3dbl_3.bn.bias\n","\t Mixed_6b.branch1x1.conv.weight\n","\t Mixed_6b.branch1x1.bn.weight\n","\t Mixed_6b.branch1x1.bn.bias\n","\t Mixed_6b.branch7x7_1.conv.weight\n","\t Mixed_6b.branch7x7_1.bn.weight\n","\t Mixed_6b.branch7x7_1.bn.bias\n","\t Mixed_6b.branch7x7_2.conv.weight\n","\t Mixed_6b.branch7x7_2.bn.weight\n","\t Mixed_6b.branch7x7_2.bn.bias\n","\t Mixed_6b.branch7x7_3.conv.weight\n","\t Mixed_6b.branch7x7_3.bn.weight\n","\t Mixed_6b.branch7x7_3.bn.bias\n","\t Mixed_6b.branch7x7dbl_1.conv.weight\n","\t Mixed_6b.branch7x7dbl_1.bn.weight\n","\t Mixed_6b.branch7x7dbl_1.bn.bias\n","\t Mixed_6b.branch7x7dbl_2.conv.weight\n","\t Mixed_6b.branch7x7dbl_2.bn.weight\n","\t Mixed_6b.branch7x7dbl_2.bn.bias\n","\t Mixed_6b.branch7x7dbl_3.conv.weight\n","\t Mixed_6b.branch7x7dbl_3.bn.weight\n","\t Mixed_6b.branch7x7dbl_3.bn.bias\n","\t Mixed_6b.branch7x7dbl_4.conv.weight\n","\t Mixed_6b.branch7x7dbl_4.bn.weight\n","\t Mixed_6b.branch7x7dbl_4.bn.bias\n","\t Mixed_6b.branch7x7dbl_5.conv.weight\n","\t Mixed_6b.branch7x7dbl_5.bn.weight\n","\t Mixed_6b.branch7x7dbl_5.bn.bias\n","\t Mixed_6b.branch_pool.conv.weight\n","\t Mixed_6b.branch_pool.bn.weight\n","\t Mixed_6b.branch_pool.bn.bias\n","\t Mixed_6c.branch1x1.conv.weight\n","\t Mixed_6c.branch1x1.bn.weight\n","\t Mixed_6c.branch1x1.bn.bias\n","\t Mixed_6c.branch7x7_1.conv.weight\n","\t Mixed_6c.branch7x7_1.bn.weight\n","\t Mixed_6c.branch7x7_1.bn.bias\n","\t Mixed_6c.branch7x7_2.conv.weight\n","\t Mixed_6c.branch7x7_2.bn.weight\n","\t Mixed_6c.branch7x7_2.bn.bias\n","\t Mixed_6c.branch7x7_3.conv.weight\n","\t Mixed_6c.branch7x7_3.bn.weight\n","\t Mixed_6c.branch7x7_3.bn.bias\n","\t Mixed_6c.branch7x7dbl_1.conv.weight\n","\t Mixed_6c.branch7x7dbl_1.bn.weight\n","\t Mixed_6c.branch7x7dbl_1.bn.bias\n","\t Mixed_6c.branch7x7dbl_2.conv.weight\n","\t Mixed_6c.branch7x7dbl_2.bn.weight\n","\t Mixed_6c.branch7x7dbl_2.bn.bias\n","\t Mixed_6c.branch7x7dbl_3.conv.weight\n","\t Mixed_6c.branch7x7dbl_3.bn.weight\n","\t Mixed_6c.branch7x7dbl_3.bn.bias\n","\t Mixed_6c.branch7x7dbl_4.conv.weight\n","\t Mixed_6c.branch7x7dbl_4.bn.weight\n","\t Mixed_6c.branch7x7dbl_4.bn.bias\n","\t Mixed_6c.branch7x7dbl_5.conv.weight\n","\t Mixed_6c.branch7x7dbl_5.bn.weight\n","\t Mixed_6c.branch7x7dbl_5.bn.bias\n","\t Mixed_6c.branch_pool.conv.weight\n","\t Mixed_6c.branch_pool.bn.weight\n","\t Mixed_6c.branch_pool.bn.bias\n","\t Mixed_6d.branch1x1.conv.weight\n","\t Mixed_6d.branch1x1.bn.weight\n","\t Mixed_6d.branch1x1.bn.bias\n","\t Mixed_6d.branch7x7_1.conv.weight\n","\t Mixed_6d.branch7x7_1.bn.weight\n","\t Mixed_6d.branch7x7_1.bn.bias\n","\t Mixed_6d.branch7x7_2.conv.weight\n","\t Mixed_6d.branch7x7_2.bn.weight\n","\t Mixed_6d.branch7x7_2.bn.bias\n","\t Mixed_6d.branch7x7_3.conv.weight\n","\t Mixed_6d.branch7x7_3.bn.weight\n","\t Mixed_6d.branch7x7_3.bn.bias\n","\t Mixed_6d.branch7x7dbl_1.conv.weight\n","\t Mixed_6d.branch7x7dbl_1.bn.weight\n","\t Mixed_6d.branch7x7dbl_1.bn.bias\n","\t Mixed_6d.branch7x7dbl_2.conv.weight\n","\t Mixed_6d.branch7x7dbl_2.bn.weight\n","\t Mixed_6d.branch7x7dbl_2.bn.bias\n","\t Mixed_6d.branch7x7dbl_3.conv.weight\n","\t Mixed_6d.branch7x7dbl_3.bn.weight\n","\t Mixed_6d.branch7x7dbl_3.bn.bias\n","\t Mixed_6d.branch7x7dbl_4.conv.weight\n","\t Mixed_6d.branch7x7dbl_4.bn.weight\n","\t Mixed_6d.branch7x7dbl_4.bn.bias\n","\t Mixed_6d.branch7x7dbl_5.conv.weight\n","\t Mixed_6d.branch7x7dbl_5.bn.weight\n","\t Mixed_6d.branch7x7dbl_5.bn.bias\n","\t Mixed_6d.branch_pool.conv.weight\n","\t Mixed_6d.branch_pool.bn.weight\n","\t Mixed_6d.branch_pool.bn.bias\n","\t Mixed_6e.branch1x1.conv.weight\n","\t Mixed_6e.branch1x1.bn.weight\n","\t Mixed_6e.branch1x1.bn.bias\n","\t Mixed_6e.branch7x7_1.conv.weight\n","\t Mixed_6e.branch7x7_1.bn.weight\n","\t Mixed_6e.branch7x7_1.bn.bias\n","\t Mixed_6e.branch7x7_2.conv.weight\n","\t Mixed_6e.branch7x7_2.bn.weight\n","\t Mixed_6e.branch7x7_2.bn.bias\n","\t Mixed_6e.branch7x7_3.conv.weight\n","\t Mixed_6e.branch7x7_3.bn.weight\n","\t Mixed_6e.branch7x7_3.bn.bias\n","\t Mixed_6e.branch7x7dbl_1.conv.weight\n","\t Mixed_6e.branch7x7dbl_1.bn.weight\n","\t Mixed_6e.branch7x7dbl_1.bn.bias\n","\t Mixed_6e.branch7x7dbl_2.conv.weight\n","\t Mixed_6e.branch7x7dbl_2.bn.weight\n","\t Mixed_6e.branch7x7dbl_2.bn.bias\n","\t Mixed_6e.branch7x7dbl_3.conv.weight\n","\t Mixed_6e.branch7x7dbl_3.bn.weight\n","\t Mixed_6e.branch7x7dbl_3.bn.bias\n","\t Mixed_6e.branch7x7dbl_4.conv.weight\n","\t Mixed_6e.branch7x7dbl_4.bn.weight\n","\t Mixed_6e.branch7x7dbl_4.bn.bias\n","\t Mixed_6e.branch7x7dbl_5.conv.weight\n","\t Mixed_6e.branch7x7dbl_5.bn.weight\n","\t Mixed_6e.branch7x7dbl_5.bn.bias\n","\t Mixed_6e.branch_pool.conv.weight\n","\t Mixed_6e.branch_pool.bn.weight\n","\t Mixed_6e.branch_pool.bn.bias\n","\t AuxLogits.conv0.conv.weight\n","\t AuxLogits.conv0.bn.weight\n","\t AuxLogits.conv0.bn.bias\n","\t AuxLogits.conv1.conv.weight\n","\t AuxLogits.conv1.bn.weight\n","\t AuxLogits.conv1.bn.bias\n","\t AuxLogits.fc.0.weight\n","\t AuxLogits.fc.0.bias\n","\t Mixed_7a.branch3x3_1.conv.weight\n","\t Mixed_7a.branch3x3_1.bn.weight\n","\t Mixed_7a.branch3x3_1.bn.bias\n","\t Mixed_7a.branch3x3_2.conv.weight\n","\t Mixed_7a.branch3x3_2.bn.weight\n","\t Mixed_7a.branch3x3_2.bn.bias\n","\t Mixed_7a.branch7x7x3_1.conv.weight\n","\t Mixed_7a.branch7x7x3_1.bn.weight\n","\t Mixed_7a.branch7x7x3_1.bn.bias\n","\t Mixed_7a.branch7x7x3_2.conv.weight\n","\t Mixed_7a.branch7x7x3_2.bn.weight\n","\t Mixed_7a.branch7x7x3_2.bn.bias\n","\t Mixed_7a.branch7x7x3_3.conv.weight\n","\t Mixed_7a.branch7x7x3_3.bn.weight\n","\t Mixed_7a.branch7x7x3_3.bn.bias\n","\t Mixed_7a.branch7x7x3_4.conv.weight\n","\t Mixed_7a.branch7x7x3_4.bn.weight\n","\t Mixed_7a.branch7x7x3_4.bn.bias\n","\t Mixed_7b.branch1x1.conv.weight\n","\t Mixed_7b.branch1x1.bn.weight\n","\t Mixed_7b.branch1x1.bn.bias\n","\t Mixed_7b.branch3x3_1.conv.weight\n","\t Mixed_7b.branch3x3_1.bn.weight\n","\t Mixed_7b.branch3x3_1.bn.bias\n","\t Mixed_7b.branch3x3_2a.conv.weight\n","\t Mixed_7b.branch3x3_2a.bn.weight\n","\t Mixed_7b.branch3x3_2a.bn.bias\n","\t Mixed_7b.branch3x3_2b.conv.weight\n","\t Mixed_7b.branch3x3_2b.bn.weight\n","\t Mixed_7b.branch3x3_2b.bn.bias\n","\t Mixed_7b.branch3x3dbl_1.conv.weight\n","\t Mixed_7b.branch3x3dbl_1.bn.weight\n","\t Mixed_7b.branch3x3dbl_1.bn.bias\n","\t Mixed_7b.branch3x3dbl_2.conv.weight\n","\t Mixed_7b.branch3x3dbl_2.bn.weight\n","\t Mixed_7b.branch3x3dbl_2.bn.bias\n","\t Mixed_7b.branch3x3dbl_3a.conv.weight\n","\t Mixed_7b.branch3x3dbl_3a.bn.weight\n","\t Mixed_7b.branch3x3dbl_3a.bn.bias\n","\t Mixed_7b.branch3x3dbl_3b.conv.weight\n","\t Mixed_7b.branch3x3dbl_3b.bn.weight\n","\t Mixed_7b.branch3x3dbl_3b.bn.bias\n","\t Mixed_7b.branch_pool.conv.weight\n","\t Mixed_7b.branch_pool.bn.weight\n","\t Mixed_7b.branch_pool.bn.bias\n","\t Mixed_7c.branch1x1.conv.weight\n","\t Mixed_7c.branch1x1.bn.weight\n","\t Mixed_7c.branch1x1.bn.bias\n","\t Mixed_7c.branch3x3_1.conv.weight\n","\t Mixed_7c.branch3x3_1.bn.weight\n","\t Mixed_7c.branch3x3_1.bn.bias\n","\t Mixed_7c.branch3x3_2a.conv.weight\n","\t Mixed_7c.branch3x3_2a.bn.weight\n","\t Mixed_7c.branch3x3_2a.bn.bias\n","\t Mixed_7c.branch3x3_2b.conv.weight\n","\t Mixed_7c.branch3x3_2b.bn.weight\n","\t Mixed_7c.branch3x3_2b.bn.bias\n","\t Mixed_7c.branch3x3dbl_1.conv.weight\n","\t Mixed_7c.branch3x3dbl_1.bn.weight\n","\t Mixed_7c.branch3x3dbl_1.bn.bias\n","\t Mixed_7c.branch3x3dbl_2.conv.weight\n","\t Mixed_7c.branch3x3dbl_2.bn.weight\n","\t Mixed_7c.branch3x3dbl_2.bn.bias\n","\t Mixed_7c.branch3x3dbl_3a.conv.weight\n","\t Mixed_7c.branch3x3dbl_3a.bn.weight\n","\t Mixed_7c.branch3x3dbl_3a.bn.bias\n","\t Mixed_7c.branch3x3dbl_3b.conv.weight\n","\t Mixed_7c.branch3x3dbl_3b.bn.weight\n","\t Mixed_7c.branch3x3dbl_3b.bn.bias\n","\t Mixed_7c.branch_pool.conv.weight\n","\t Mixed_7c.branch_pool.bn.weight\n","\t Mixed_7c.branch_pool.bn.bias\n","\t fc.0.weight\n","\t fc.0.bias\n","\t fc.2.weight\n","\t fc.2.bias\n"]}],"source":["# Send the model to GPU\n","model_ft = model_ft.to(device)\n","\n","# Gather the parameters to be optimized/updated in this run. If we are\n","#  finetuning we will be updating all parameters. However, if we are\n","#  doing feature extract method, we will only update the parameters\n","#  that we have just initialized, i.e. the parameters with requires_grad\n","#  is True.\n","params_to_update = model_ft.parameters()\n","print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.Adam(params_to_update, lr=learningRate)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVJl5AvUqK13"},"outputs":[],"source":["def test_model(model,dataloaders,device, phaseKey = 'val'):\n","    classStats = [{\n","        'fn': 0,\n","        'tn': 0,\n","        'tp': 0,\n","        'fp': 0,\n","        'n': 0,\n","    } for i in range(num_classes)]\n","    correctlyPredicted = 0\n","    n = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for inputs, labels in dataloaders[phaseKey]:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            # Iteramos para chequear estadisticas\n","            for i, correctClass in enumerate(labels.data):\n","              n += 1\n","              predictedClass = int(preds[i].item())\n","              correctClass = int(correctClass.item())\n","              classStats[correctClass]['n'] += 1\n","              if correctClass == predictedClass:\n","                  correctlyPredicted += 1\n","                  classStats[correctClass]['tp'] += 1\n","                  for j in range(num_classes):\n","                      if j != correctClass:\n","                          classStats[j]['tn'] += 1\n","              else:\n","                  classStats[correctClass]['fn'] += 1\n","                  classStats[predictedClass]['fp'] += 1\n","                  for j in range(num_classes):\n","                      if j != correctClass and j != predictedClass:\n","                          classStats[j]['tn'] += 1\n","    accuracy = correctlyPredicted * 1.0 / n\n","    return classStats, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aK3auwSS05Le"},"outputs":[],"source":["def printClassStats(stats, f = None):\n","  recall = sensitivity = stats['tp'] / (stats['tp'] + stats['fn']) # prob positive test result\n","  specificity = stats['tn'] / (stats['tn'] + stats['fp'])          # prob negative test result\n","  if stats['tp'] + stats['fp'] > 0:\n","    precision = stats['tp'] / (stats['tp'] + stats['fp'])          # prob of recognized positive actually correct\n","  else:\n","    precision = 1\n","    printFile(\"Setting precision as 1 but no positive value has been reported, so this is placeholder\", f)\n","  if precision + recall == 0:\n","    printFile(\"Setting f1 as 0 because precision + recall is ZERO\", f)\n","    f1 = 0.0\n","  else:\n","    f1 = 2 * (precision * recall) / ( precision + recall )\n","  printFile(\"Sensitivity (%): \" + str(round(sensitivity * 100)), f)\n","  printFile(\"Specificity (%): \" + str(round(specificity * 100)), f)\n","  printFile(\"Precision  (%): \" + str(round(precision * 100)), f)\n","  printFile(\"F1 Score  (%): \" + str(round(f1 * 100)), f)\n","  printFile(\"Number of images: \" + str(stats['n']), f)\n","  return recall, specificity, precision, f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D84am68jwlX6","scrolled":false},"outputs":[],"source":["accuracyValues = []\n","adStatValues = []\n","cnStatValues = []\n","mciStatValues = []\n","def trainAndPlot():\n","  global model_ft\n","  for i in range(0, executions):\n","      experimentExecutionName = experimentName + '_' + str(i)\n","      print(\"--- Execution \" + str(i) + \" begin ---\")\n","      # Setup the loss fxn\n","      crossEntrophyWeigths2 = crossEntrophyWeigths.to(device)\n","      criterion = nn.CrossEntropyLoss(crossEntrophyWeigths2)\n","\n","      logFile = os.path.join(experimentOutputFolder, experimentExecutionName + '_train.log')\n","\n","      # Train and evaluate\n","      model_ft, val_acc_hist, val_loss_hist, train_acc_hist, train_loss_hist, val_selection_criteria_hist, train_selection_criteria_hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"), logFile = logFile, selection_criteria = selectCriteria, save_all_epochs = calculateAUCROC, experimentExecutionName = experimentExecutionName)\n","\n","      torch.save(model_ft.state_dict(), os.path.join(experimentOutputFolder, experimentExecutionName + '.pth'))\n","\n","      # validation accuracy\n","      fig = plt.figure()\n","      lst = [ x.cpu().item() for x in val_acc_hist ]\n","      plt.plot(lst)\n","      ax = plt.gca()\n","      plt.text(0.05, 0.9, 'FE = ' + str(feature_extract), transform=ax.transAxes)\n","      plt.text(0.05, 0.8, 'LR = ' + str(learningRate), transform=ax.transAxes)\n","      plt.text(0.05, 0.7, 'batch = ' + str(batch_size), transform=ax.transAxes)\n","      plt.suptitle(experimentExecutionName + ' (acc set de validacion)')\n","      plt.ylabel('Accuracy')\n","      plt.xlabel('Epochs')\n","      plt.savefig(os.path.join(experimentOutputFolder, experimentExecutionName + '_val_acc.png'))\n","      plt.clf()\n","\n","      # validation loss\n","      fig = plt.figure()\n","      plt.plot(val_loss_hist)\n","      ax = plt.gca()\n","      plt.text(0.05, 0.3, 'FE = ' + str(feature_extract), transform=ax.transAxes)\n","      plt.text(0.05, 0.2, 'LR = ' + str(learningRate), transform=ax.transAxes)\n","      plt.text(0.05, 0.1, 'batch = ' + str(batch_size), transform=ax.transAxes)\n","      plt.suptitle(experimentExecutionName + ' (loss set de validacion)')\n","      plt.ylabel('Loss')\n","      plt.xlabel('Epochs')\n","      plt.savefig(os.path.join(experimentOutputFolder, experimentExecutionName + '_val_loss.png'))\n","      plt.clf()\n","\n","      # train accuracy\n","      fig = plt.figure()\n","      lst = [ x.cpu().item() for x in train_acc_hist ]\n","      ax = plt.gca()\n","      plt.text(0.05, 0.9, 'FE = ' + str(feature_extract), transform=ax.transAxes)\n","      plt.text(0.05, 0.8, 'LR = ' + str(learningRate), transform=ax.transAxes)\n","      plt.text(0.05, 0.7, 'batch = ' + str(batch_size), transform=ax.transAxes)\n","      plt.plot(lst)\n","      plt.suptitle(experimentExecutionName + ' (acc set de train)')\n","      plt.ylabel('Accuracy')\n","      plt.xlabel('Epochs')\n","      plt.savefig(os.path.join(experimentOutputFolder, experimentExecutionName + '_train_acc.png'))\n","      plt.clf()\n","\n","      # train loss\n","      fig = plt.figure()\n","      ax = plt.gca()\n","      plt.text(0.05, 0.3, 'FE = ' + str(feature_extract), transform=ax.transAxes)\n","      plt.text(0.05, 0.2, 'LR = ' + str(learningRate), transform=ax.transAxes)\n","      plt.text(0.05, 0.1, 'batch = ' + str(batch_size), transform=ax.transAxes)\n","      plt.plot(train_loss_hist)\n","      plt.suptitle(experimentExecutionName + ' (Loss set de train)')\n","      plt.ylabel('Loss')\n","      plt.xlabel('Epochs')\n","      plt.savefig(os.path.join(experimentOutputFolder, experimentExecutionName + '_train_loss.png'))\n","      plt.clf()\n","\n","      # validation selection criteria\n","      abbrv = selectCriteriaAbbrv\n","      fig = plt.figure()\n","      lst = [ x for x in val_selection_criteria_hist ]\n","      plt.plot(lst)\n","      ax = plt.gca()\n","      plt.text(0.05, 0.9, 'FE = ' + str(feature_extract), transform=ax.transAxes)\n","      plt.text(0.05, 0.8, 'LR = ' + str(learningRate), transform=ax.transAxes)\n","      plt.text(0.05, 0.7, 'batch = ' + str(batch_size), transform=ax.transAxes)\n","      plt.suptitle(experimentExecutionName + ' ('+abbrv+' set de validacion)')\n","      plt.ylabel(selectCriteria)\n","      plt.xlabel('Epochs')\n","      plt.savefig(os.path.join(experimentOutputFolder, experimentExecutionName + '_val_'+abbrv+'.png'))\n","      plt.clf()\n","\n","      # train selection criteria\n","      fig = plt.figure()\n","      lst = [ x for x in train_selection_criteria_hist ]\n","      ax = plt.gca()\n","      plt.text(0.05, 0.9, 'FE = ' + str(feature_extract), transform=ax.transAxes)\n","      plt.text(0.05, 0.8, 'LR = ' + str(learningRate), transform=ax.transAxes)\n","      plt.text(0.05, 0.7, 'batch = ' + str(batch_size), transform=ax.transAxes)\n","      plt.plot(lst)\n","      plt.suptitle(experimentExecutionName + ' ('+abbrv+' set de train)')\n","      plt.ylabel(selectCriteria)\n","      plt.xlabel('Epochs')\n","      plt.savefig(os.path.join(experimentOutputFolder, experimentExecutionName + '_train_'+abbrv+'.png'))\n","      plt.clf()\n","\n","      stats, accuracy = test_model(model_ft, dataloaders_dict, device)\n","\n","      print(\"accuracy: \" + str(accuracy))\n","      accuracyValues.append(accuracy)\n","\n","      f = open(os.path.join(experimentOutputFolder, experimentExecutionName + \"_stats.txt\"), \"w\")\n","      title = \"CN\" if num_classes == 3 else \"no AD\"\n","      printFile(title + \" stats: \", f)\n","      recall, specificity, precision, f1 = printClassStats(stats[0], f)\n","      cnStatValues.append({\n","          \"recall\": recall,\n","          \"specificity\": specificity,\n","          \"precision\": precision,\n","          \"f1\": f1\n","      })\n","      # AD\n","      printFile(\"\\nAD stats: \", f)\n","      recall, specificity, precision, f1 = printClassStats(stats[1], f)\n","      adStatValues.append({\n","          \"recall\": recall,\n","          \"specificity\": specificity,\n","          \"precision\": precision,\n","          \"f1\": f1\n","      })\n","      if num_classes == 3:\n","        # MCI\n","        printFile(\"\\nMCI stats: \", f)\n","        recall, specificity, precision, f1 = printClassStats(stats[2], f)\n","        mciStatValues.append({\n","            \"recall\": recall,\n","            \"specificity\": specificity,\n","            \"precision\": precision,\n","            \"f1\": f1\n","        })\n","      f.close()\n","\n","      print(\"--- Execution End ---\")    "]},{"cell_type":"code","source":["if doTrain:\n","  trainAndPlot()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"G5adv7U46uPA","executionInfo":{"status":"ok","timestamp":1679327744101,"user_tz":180,"elapsed":576220,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}},"outputId":"595af70f-3f70-4361-e2b8-be91248e7427"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Execution 0 begin ---\n","Epoch 0/24\n","----------\n","train Loss: 0.4148 Acc: 0.6698 f1AD: 0.0189\n","\u0001\n","val Loss: 0.2616 Acc: 0.6667 f1AD: 0.0000\n","Epoch 1/24\n","----------\n","\u0001\n","train Loss: 0.2683 Acc: 0.6667 f1AD: 0.0000\n","\u0001\n","val Loss: 0.2513 Acc: 0.6667 f1AD: 0.0000\n","Epoch 2/24\n","----------\n","train Loss: 0.1805 Acc: 0.7079 f1AD: 0.2269\n","val Loss: 0.2479 Acc: 0.6933 f1AD: 0.2203\n","Epoch 3/24\n","----------\n","train Loss: 0.1182 Acc: 0.8730 f1AD: 0.7647\n","val Loss: 0.2839 Acc: 0.7600 f1AD: 0.5814\n","Epoch 4/24\n","----------\n","train Loss: 0.1001 Acc: 0.9365 f1AD: 0.8964\n","val Loss: 0.4298 Acc: 0.7433 f1AD: 0.6244\n","Epoch 5/24\n","----------\n","train Loss: 0.0903 Acc: 0.9397 f1AD: 0.9026\n","val Loss: 0.4278 Acc: 0.7633 f1AD: 0.5799\n","Epoch 6/24\n","----------\n","train Loss: 0.0973 Acc: 0.9381 f1AD: 0.9008\n","val Loss: 0.3514 Acc: 0.7567 f1AD: 0.5350\n","Epoch 7/24\n","----------\n","train Loss: 0.0732 Acc: 0.9603 f1AD: 0.9370\n","val Loss: 0.4396 Acc: 0.7433 f1AD: 0.4615\n","Epoch 8/24\n","----------\n","train Loss: 0.0599 Acc: 0.9651 f1AD: 0.9453\n","val Loss: 0.5813 Acc: 0.7900 f1AD: 0.6802\n","Epoch 9/24\n","----------\n","train Loss: 0.0301 Acc: 0.9857 f1AD: 0.9782\n","val Loss: 0.4722 Acc: 0.7300 f1AD: 0.4636\n","Epoch 10/24\n","----------\n","train Loss: 0.0503 Acc: 0.9667 f1AD: 0.9481\n","val Loss: 0.5122 Acc: 0.7500 f1AD: 0.5283\n","Epoch 11/24\n","----------\n","train Loss: 0.0519 Acc: 0.9698 f1AD: 0.9533\n","val Loss: 0.6202 Acc: 0.7800 f1AD: 0.6916\n","Epoch 12/24\n","----------\n","train Loss: 0.0401 Acc: 0.9698 f1AD: 0.9531\n","val Loss: 0.7109 Acc: 0.7700 f1AD: 0.6425\n","Epoch 13/24\n","----------\n","train Loss: 0.0440 Acc: 0.9683 f1AD: 0.9507\n","val Loss: 0.6057 Acc: 0.7667 f1AD: 0.6500\n","Epoch 14/24\n","----------\n","train Loss: 0.0367 Acc: 0.9825 f1AD: 0.9734\n","val Loss: 0.5547 Acc: 0.7500 f1AD: 0.5399\n","Epoch 15/24\n","----------\n","train Loss: 0.0339 Acc: 0.9841 f1AD: 0.9757\n","val Loss: 0.7087 Acc: 0.7733 f1AD: 0.6634\n","Epoch 16/24\n","----------\n","train Loss: 0.0190 Acc: 0.9921 f1AD: 0.9880\n","val Loss: 0.6318 Acc: 0.7267 f1AD: 0.4810\n","Epoch 17/24\n","----------\n","train Loss: 0.0099 Acc: 0.9937 f1AD: 0.9904\n","val Loss: 0.7066 Acc: 0.7567 f1AD: 0.5922\n","Epoch 18/24\n","----------\n","train Loss: 0.0096 Acc: 0.9937 f1AD: 0.9904\n","val Loss: 0.8404 Acc: 0.7833 f1AD: 0.6734\n","Epoch 19/24\n","----------\n","train Loss: 0.0232 Acc: 0.9873 f1AD: 0.9807\n","val Loss: 0.7207 Acc: 0.7567 f1AD: 0.5466\n","Epoch 20/24\n","----------\n","train Loss: 0.0309 Acc: 0.9857 f1AD: 0.9783\n","val Loss: 0.6856 Acc: 0.7267 f1AD: 0.4744\n","Epoch 21/24\n","----------\n","train Loss: 0.0264 Acc: 0.9841 f1AD: 0.9756\n","val Loss: 1.0760 Acc: 0.7600 f1AD: 0.6842\n","Epoch 22/24\n","----------\n","train Loss: 0.0157 Acc: 0.9937 f1AD: 0.9904\n","val Loss: 0.7966 Acc: 0.7633 f1AD: 0.5896\n","Epoch 23/24\n","----------\n","train Loss: 0.0121 Acc: 0.9921 f1AD: 0.9880\n","val Loss: 1.0610 Acc: 0.7633 f1AD: 0.6787\n","Epoch 24/24\n","----------\n","train Loss: 0.0099 Acc: 0.9952 f1AD: 0.9928\n","val Loss: 0.8654 Acc: 0.7667 f1AD: 0.6500\n","Training complete in 9m 28s\n","Best val f1AD: 0.691589\n","accuracy: 0.78\n","no AD stats: \n","Sensitivity (%): 80\n","Specificity (%): 74\n","Precision  (%): 86\n","F1 Score  (%): 83\n","Number of images: 200\n","\n","AD stats: \n","Sensitivity (%): 74\n","Specificity (%): 80\n","Precision  (%): 65\n","F1 Score  (%): 69\n","Number of images: 100\n","--- Execution End ---\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"xA-ShjPRUYxP"},"source":["# ROC - AUC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAU7r7yJ9tU2"},"outputs":[],"source":["# Source: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n","def calculateAUCROCs(dataloader, model, validationSet, labels = [0,1,2], labelDict = {\n","    0: \"CN\",\n","    1: \"AD\",\n","    2: \"MCI\"\n","  }, epoch = 0, executionNumber = 0):\n","  testY, predY = collectAllData(dataloader, model)\n","\n","  #print(testY)\n","  #print(predY)\n","\n","  ns_probs = np.empty( shape = predY.shape )\n","  ns_probs[:, :] = 1.0/len(labels)\n","\n","  # calculate scores\n","  ns_auc = roc_auc_score(testY, ns_probs, multi_class='ovr', labels= labels, average = None)\n","  lr_auc = roc_auc_score(testY, predY, multi_class='ovr', labels= labels, average = None)\n","\n","  f = open(os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch)  + '_' + validationSet + '_stats.txt'), \"w\")\n","  f.write(str(lr_auc))\n","  f.close()\n","\n","  # Por separado\n","  for i in labels:\n","    ns_fpr, ns_tpr, _ = roc_curve(testY, ns_probs[:, i], pos_label = i) \n","    lr_fpr, lr_tpr, _ = roc_curve(testY, predY[:, i], pos_label = i)\n","\n","    # plot the roc curve for the model\n","    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No ' + labelDict[i])\n","    pyplot.plot(lr_fpr, lr_tpr, marker='.', label=labelDict[i])\n","    # axis labels\n","    pyplot.xlabel('False Positive Rate')\n","    pyplot.ylabel('True Positive Rate')\n","    # show the legend\n","    pyplot.legend()\n","    # show the plot\n","    # pyplot.show()\n","    filename = os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch) + '_' + validationSet + '_roc_'+labelDict[i]+'.png')\n","    #print(\"Generando \" + filename)\n","    pyplot.savefig(filename)\n","\n","    pyplot.clf()\n","\n","  # Este une los tres gráficos en uno\n","  middleLinePlotted = False\n","  for i in labels:\n","    ns_fpr, ns_tpr, _ = roc_curve(testY, ns_probs[:, i], pos_label = i) \n","    lr_fpr, lr_tpr, _ = roc_curve(testY, predY[:, i], pos_label = i)\n","\n","    # plot the roc curve for the model\n","    if not middleLinePlotted:\n","      pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='')\n","      middleLinePlotted = True\n","    pyplot.plot(lr_fpr, lr_tpr, marker='.', label=labelDict[i])\n","    # axis labels\n","    pyplot.xlabel('False Positive Rate')\n","    pyplot.ylabel('True Positive Rate')\n","    # show the legend\n","    pyplot.legend()\n","    # show the plot\n","    # pyplot.show()\n","    filename = os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch) + '_' + validationSet + '_roc.png')\n","    #print(\"Generando \" + filename)\n","    pyplot.savefig(filename)\n","\n","  pyplot.clf()\n","\n","  return lr_auc"]},{"cell_type":"code","source":["# Source: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n","def calculateAUCROCs2Classes(dataloader, model, validationSet = \"valFleni\", labels = [1,2],   labelDict = {\n","    0: \"CN\",\n","    1: \"AD\",\n","    2: \"MCI\"\n","  }, epoch = 0, executionNumber = 0):\n","  testY, predY = collectAllData(dataloader, model)\n","\n","  #print(testY)\n","  #print(predY)\n","\n","  ns_probs = np.empty( shape = predY.shape[0] )\n","  ns_probs[:] = 0.5\n","\n","  # calculate scores\n","  ns_auc = roc_auc_score(testY, ns_probs, labels= labels, average = None)\n","  lr_auc = roc_auc_score(testY, predY[:, 1], labels= labels, average = None) # AD\n","\n","  f = open(os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch)  + '_' + validationSet + '_stats.txt'), \"w\")\n","  f.write(str(lr_auc))\n","  f.close()\n","\n","  # Un solo gráfico\n","  ns_fpr, ns_tpr, _ = roc_curve(testY, ns_probs, pos_label = 1) \n","  lr_fpr, lr_tpr, _ = roc_curve(testY, predY[:, 1], pos_label = 1)\n","\n","  pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='')\n","  pyplot.plot(lr_fpr, lr_tpr, marker='.', label=\"AD\")\n","  # axis labels\n","  pyplot.xlabel('False Positive Rate')\n","  pyplot.ylabel('True Positive Rate')\n","  # show the legend\n","  pyplot.legend()\n","  # show the plot\n","  # pyplot.show()\n","  filename = os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(epoch) + '_' + validationSet + '_roc.png')\n","  #print(\"Generando \" + filename)\n","  pyplot.savefig(filename)\n","\n","  pyplot.clf()\n","\n","  return lr_auc"],"metadata":{"id":"Acg7BS2bZwwv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Esi1qDfbjNY7"},"outputs":[],"source":["# Returns testY and predY for a data loader\n","def collectAllData(dataloader, model, classNumber = 1):\n","  testY = np.empty((0,))\n","  predY = np.empty((0,num_classes))\n","  model.eval()\n","  with torch.no_grad():\n","    for inputs, labels in dataloader:\n","      inputs = inputs.to(device)\n","\n","      outputs = model(inputs)\n","\n","      soft_outputs = torch.nn.functional.softmax(outputs, dim=1)\n","      soft_outputs = soft_outputs.cpu().detach().numpy()\n","\n","      testY = np.concatenate([testY, labels.detach().numpy()])\n","      predY = np.concatenate([predY, soft_outputs], axis = 0) \n","\n","      torch.cuda.empty_cache()\n","\n","  return testY, predY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuiTfuCMd8j4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679327744103,"user_tz":180,"elapsed":9,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}},"outputId":"8accf074-0b47-4051-f2e0-8fd52e9743fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final stats: \n","Executions: 1\n","Accuracy mean: 0.7799999713897705\n","Accuracy std: nan\n","Best accuracy: 0.7799999713897705\n","Worst accuracy: 0.7799999713897705\n"]}],"source":["accuracyValues = torch.tensor(accuracyValues)\n","std, mean = torch.std_mean(accuracyValues)\n","f = open(os.path.join(\n","    os.path.join(experimentOutputFolder, experimentName + '_results.txt')), \"w\")\n","printFile(\"Final stats: \", f)\n","printFile(\"Executions: \" + str(executions), f)\n","printFile(\"Accuracy mean: \" + str(mean.item()), f)\n","printFile(\"Accuracy std: \" + str(std.item()), f)\n","printFile(\"Best accuracy: \" + str(accuracyValues.max().item()), f)\n","printFile(\"Worst accuracy: \" + str(accuracyValues.min().item()), f)\n","f.close()"]},{"cell_type":"markdown","source":["## AUC/ROC ADNI"],"metadata":{"id":"glTkQo0JS6vU"}},{"cell_type":"code","source":["def processAUCROCThreeClases(dataloader, dlName = \"val\", labels = [0, 1,2], labelDict = {\n","    0: \"CN\",\n","    1: \"AD\",\n","    2: \"MCI\"\n","  }):\n","  bestAD = 0.0\n","  bestMCI = 0.0\n","  bestCN = 0.0\n","  bestADEpoch = -1\n","  bestMCIEpoch = -1\n","  bestCNEpoch = -1\n","  executionNumber = 0\n","  for i in range(0, num_epochs):\n","    model_state_dict = torch.load(os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(i) + '.pth'), map_location=device)\n","    model_ft.load_state_dict(model_state_dict)\n","    auc_rocs = calculateAUCROCs(dataloader, model_ft, dlName, executionNumber = executionNumber, epoch = i, labels = labels, labelDict = labelDict)\n","    print(\"CN: %.3f AD: %.3f MCI: %.3f\" % (auc_rocs[0], auc_rocs[1], auc_rocs[2]))\n","    if auc_rocs[0] > bestCN:\n","      bestCN = auc_rocs[0]\n","      bestCNEpoch = i\n","    if auc_rocs[1] > bestAD:\n","      bestAD = auc_rocs[1]\n","      bestADEpoch = i\n","    if auc_rocs[2] > bestMCI:\n","      bestMCI = auc_rocs[2]\n","      bestMCIEpoch = i\n","\n","  print(\"Best CN: %.3f Epoch: %d\" % (bestCN, bestCNEpoch))\n","  print(\"Best AD: %.3f Epoch: %d\" % (bestAD, bestADEpoch))\n","  print(\"Best MCI: %.3f Epoch: %d\" % (bestMCI, bestMCIEpoch))"],"metadata":{"id":"4dIumF1LeXkf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def processAUCROCTwoClases(dataloader, dlName, labels = [0, 1,2], labelDict = {\n","    0: \"CN\",\n","    1: \"AD\",\n","    2: \"MCI\"\n","  }):\n","  bestAD = 0.0\n","  bestMCI = 0.0\n","  bestCN = 0.0\n","  bestADEpoch = -1\n","  bestMCIEpoch = -1\n","  bestCNEpoch = -1\n","  executionNumber = 0\n","  for i in range(0, num_epochs):\n","    model_state_dict = torch.load(os.path.join(experimentOutputFolder, experimentName + '_' + str(executionNumber) + '_epoch' + str(i) + '.pth'), map_location=device)\n","    model_ft.load_state_dict(model_state_dict)\n","    auc_rocs = calculateAUCROCs2Classes(dataloader, model_ft, dlName, executionNumber = executionNumber, epoch = i, labels = labels, labelDict = labelDict)\n","    print(\"AD: %.3f\" % (auc_rocs))\n","    if auc_rocs > bestAD:\n","      bestAD = auc_rocs\n","      bestADEpoch = i\n","\n","  print(\"Best AD: %.3f Epoch: %d\" % (bestAD, bestADEpoch))"],"metadata":{"id":"kMXl__RYeire"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":499},"id":"Rav0sPdP_tKj","executionInfo":{"status":"ok","timestamp":1679327923415,"user_tz":180,"elapsed":179318,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}},"outputId":"dc0bab3f-c480-4968-f5b5-8b668cc95b18"},"outputs":[{"output_type":"stream","name":"stdout","text":["AD: 0.740\n","AD: 0.782\n","AD: 0.816\n","AD: 0.812\n","AD: 0.788\n","AD: 0.791\n","AD: 0.834\n","AD: 0.833\n","AD: 0.852\n","AD: 0.832\n","AD: 0.830\n","AD: 0.832\n","AD: 0.826\n","AD: 0.834\n","AD: 0.828\n","AD: 0.816\n","AD: 0.806\n","AD: 0.815\n","AD: 0.813\n","AD: 0.809\n","AD: 0.798\n","AD: 0.811\n","AD: 0.803\n","AD: 0.810\n","AD: 0.808\n","Best AD: 0.852 Epoch: 8\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}}],"source":["if num_classes == 3:\n","  processAUCROCThreeClases(dataloaders_dict['val'], \"val\")\n","else:\n","  processAUCROCTwoClases(dataloaders_dict['val'], \"val\", labels = [0,1], labelDict = { 0: \"noAD\", 1: \"AD\" })"]},{"cell_type":"markdown","source":["## AUC/ROC Fleni"],"metadata":{"id":"UD1MpynaS9W-"}},{"cell_type":"code","source":["if num_classes == 3:\n","  labels = [1,2]\n","  labelDict = {\n","    0: \"CN\",\n","    1: \"AD\",\n","    2: \"MCI\"\n","  }\n","else:\n","  labels = [0,1]\n","  labelDict = { 0: \"noAD\", 1: \"AD\" }\n","\n","processAUCROCTwoClases(dataloaders_dict['valFleni'], \"valFleni\", labels = labels, labelDict = labelDict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":499},"id":"OI7ug86_TBEz","executionInfo":{"status":"ok","timestamp":1679328001300,"user_tz":180,"elapsed":77899,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}},"outputId":"61fc7c23-c45e-4d05-9881-4df452895a2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","AD: 0.500\n","Best AD: 0.500 Epoch: 0\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"I_EV7l-gRDhd"},"source":["# Fleni Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZDOvHdNREC3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679328004556,"user_tz":180,"elapsed":3262,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}},"outputId":"9002ab07-1932-47f7-ed84-1f7a12aac5c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing Datasets and Dataloaders...\n","accuracy fleni: 0.39285714285714285\n","\n","AD stats: \n","Setting precision as 1 but no positive value has been reported, so this is placeholder\n","Sensitivity (%): 0\n","Specificity (%): 100\n","Precision  (%): 100\n","F1 Score  (%): 0\n","Number of images: 68\n","\n","MCI stats: \n","Sensitivity (%): 100\n","Specificity (%): 0\n","Precision  (%): 39\n","F1 Score  (%): 56\n","Number of images: 44\n"]}],"source":["trainGridArgs = {\n","    \n","}\n","valGridArgs = {}\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        TransformGridImage(**dataAugmentation),\n","        transforms.ToTensor(),\n","        transforms.Normalize(normalization[\"trainMeans\"], normalization[\"trainStds\"])\n","    ]),\n","    'val': transforms.Compose([\n","        TransformGridImage(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(normalization[\"trainMeans\"], normalization[\"trainStds\"])\n","    ]),\n","}\n","\n","print(\"Initializing Datasets and Dataloaders...\")\n","\n","# Create training and validation datasets\n","\n","image_datasets = {\n","    #'train': ADNIDataset('trainDL', trainDatasetCSV, imagesFolder, transform = data_transforms['train'], target_transform =ToLabelOutput(), cacheSize = trainCacheSize),\n","    #'val': ADNIDataset('valDL', valDatasetCSV, imagesFolder, transform = data_transforms['val'], target_transform =ToLabelOutput(), cacheSize = validationCacheSize ),\n","    'valFleni': FleniMyriamDataset('valFleniDL', fleniValDatasetCSV, fleniImagesFolder, transform = data_transforms['val'], target_transform =ToLabelOutputFleni(num_classes), cacheSize = validationCacheSize ),\n","}\n","\n","# Create training and validation dataloaders\n","dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=dl_num_workers) for x in ['valFleni']}\n","\n","# Detect if we have a GPU available\n","device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n","\n","device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n","#model_state_dict = torch.load(os.path.join(experimentOutputFolder, experimentName + '_0.pth'), map_location=device)\n","#model_ft.load_state_dict(model_state_dict)\n","stats, accuracy = test_model(model_ft, dataloaders_dict, device, 'valFleni')\n","\n","print(\"accuracy fleni: \" + str(accuracy))\n","\n","f = open(os.path.join(experimentOutputFolder, \"fleni_stats.txt\"), \"w\")\n","# f = None\n","# AD\n","printFile(\"\\nAD stats: \", f)\n","recall, specificity, precision, f1 = printClassStats(stats[1])\n","#adStatValues.append({\n","#        \"recall\": recall,\n","#        \"specificity\": specificity,\n","#        \"precision\": precision,\n","#        \"f1\": f1\n","#})\n","# MCI\n","mciIndex = 2 if num_classes == 3 else 0 \n","printFile(\"\\nMCI stats: \", f)\n","recall, specificity, precision, f1 = printClassStats(stats[mciIndex])\n","#mciStatValues.append({\n","#        \"recall\": recall,\n","#        \"specificity\": specificity,\n","#        \"precision\": precision,\n","#        \"f1\": f1\n","#    })\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMzkst1OSjWO"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1XPT5bY_VeoHiPXKBYY3ssvguYQ_SCKvV","timestamp":1679323000466},{"file_id":"18HO3TZMDYyQx3jZtrrrdP3bZqcSPEmnd","timestamp":1679061854283},{"file_id":"1sPTNZIQkYFpsG0drQ6Qyslz6tzPbDhT7","timestamp":1678742445183},{"file_id":"1Qo_DZyfNBZnp7EhUN3zbw65mSAAy-9xU","timestamp":1678453853686}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}