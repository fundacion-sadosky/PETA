{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2596,"status":"ok","timestamp":1676057570244,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"},"user_tz":180},"id":"xhN8vz1PQ8Wv","outputId":"3ac07a08-c14a-4006-df9e-a8f1f1a3c2fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vpyelL4yXvC0","executionInfo":{"status":"ok","timestamp":1676057570244,"user_tz":180,"elapsed":4,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"ChL61Twesn0o"},"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1749,"status":"ok","timestamp":1676057571990,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"},"user_tz":180},"id":"sRvvbIrxP5Hd","outputId":"1a33bf5d-228d-4eac-f78f-afa1bfe3764d"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch Version:  1.13.1+cu116\n","Torchvision Version:  0.14.1+cu116\n"]}],"source":["from __future__ import print_function, division\n","import os\n","import time\n","import copy\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from torchvision import transforms, utils, models, datasets\n","import torch.nn as nn\n","import torch.optim as optim\n","import nibabel as nib\n","import scipy.ndimage as ndi\n","from pathlib import Path\n","from PIL import Image\n","import io\n","import json\n","import random\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"HErqZ9GNCDq6","executionInfo":{"status":"ok","timestamp":1676057571990,"user_tz":180,"elapsed":4,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["imagesFolder = '/content/gdrive/MyDrive/Tesis/Imagenes/ADNI-MUESTRA-930'\n","trainDatasetCSV = imagesFolder + '/MUESTRA_train.csv'\n","valDatasetCSV =   imagesFolder + '/MUESTRA_val.csv'\n","experimentName = 'Muestra930_1_Full'\n","experimentOutputFolder = '/content/gdrive/MyDrive/Tesis/Experimentos/muestra930_1'\n","experimentDescription = 'Muestra930 sin feature extract'\n","executions = 1"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"VYNyT00wpzwU","executionInfo":{"status":"ok","timestamp":1676057571991,"user_tz":180,"elapsed":4,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n","model_name = \"inception\"\n","\n","# Number of classes in the dataset\n","num_classes = 3\n","\n","# Batch size for training (change depending on how much memory you have)\n","batch_size = 10\n","\n","# Number of epochs to train for\n","num_epochs = 50\n","\n","# Flag for feature extracting. When False, we finetune the whole model,\n","#   when True we only update the reshaped layer params\n","feature_extract = True\n","\n","usePretrained = True\n","\n","learningRate = 0.0001\n","\n","crossEntrophyWeigths = torch.tensor([1.0,1.0,1.0])\n","\n","slicesToCut = 0\n","\n","# Data augmentation\n","dataAugmentation = {\n","    \"angle\": 13.0,\n","    \"zoom\": 0.15,\n","    \"shiftX\": 10.0,\n","    \"shiftY\": 10.0,\n","    \"angleTransformChance\": 0.1,\n","    \"zoomTransformChance\": 0.1,\n","    \"shiftTransformChance\": 0.0\n","}\n","dataAugmentation = {}\n","\n","validationCacheSize = 400\n","trainCacheSize = 700"]},{"cell_type":"code","source":["f = open(os.path.join(experimentOutputFolder, experimentName + \"_params.txt\"), \"w\")\n","f.write(\"batch_size: \" + str(batch_size) + \"\\n\")\n","f.write(\"epochs: \" + str(num_epochs) + \"\\n\")\n","f.write(\"feature_extract: \" + str(feature_extract) + \"\\n\")\n","f.write(\"usePretrained: \" + str(usePretrained) + \"\\n\")\n","f.write(\"learningRate: \" + str(learningRate) + \"\\n\")\n","f.write(\"cross entrophy weights: \" + str(crossEntrophyWeigths) + \"\\n\")\n","f.write(\"slicesToCut: \" + str(slicesToCut) + \"\\n\")\n","f.write(\"dataAugmentation: \" + str(json.dumps(dataAugmentation)) + \"\\n\")\n","f.write(\"executions: \" + str(executions) + \"\\n\")\n","f.write(\"validationCacheSize: \" + str(validationCacheSize) + \"\\n\")\n","f.write(\"trainCacheSize: \" + str(trainCacheSize) + \"\\n\")\n","f.close()"],"metadata":{"id":"a2ZFcEXjwezm","executionInfo":{"status":"ok","timestamp":1676057572572,"user_tz":180,"elapsed":585,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["f = open(os.path.join(experimentOutputFolder, experimentName + \"_descripcion.txt\"), \"w\")\n","f.write(experimentDescription)\n","f.close()"],"metadata":{"id":"zJYNgYD9AC4X","executionInfo":{"status":"ok","timestamp":1676057572572,"user_tz":180,"elapsed":6,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"VDba_NBLQOou","executionInfo":{"status":"ok","timestamp":1676057572572,"user_tz":180,"elapsed":6,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["# https://stackoverflow.com/questions/8598673/how-to-save-a-pylab-figure-into-in-memory-file-which-can-be-read-into-pil-image\n","def fig2img(fig):\n","    \"\"\"Convert a Matplotlib figure to a PIL Image and return it\"\"\"\n","    buf = io.BytesIO()\n","    fig.savefig(buf, facecolor='black', dpi = 64, transparent=False) # dpi Requerido para que la imagen sea 512x512\n","    buf.seek(0)\n","    img = Image.open(buf)\n","    return img"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"K-Z1h8wrQaiB","executionInfo":{"status":"ok","timestamp":1676057572574,"user_tz":180,"elapsed":8,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["def clipped_zoom(img, zoom_factor, **kwargs):\n","\n","    h, w = img.shape[:2]\n","\n","    # For multichannel images we don't want to apply the zoom factor to the RGB\n","    # dimension, so instead we create a tuple of zoom factors, one per array\n","    # dimension, with 1's for any trailing dimensions after the width and height.\n","    zoom_tuple = (zoom_factor,) * 2 + (1,) * (img.ndim - 2)\n","\n","    # Zooming out\n","    if zoom_factor < 1:\n","\n","        # Bounding box of the zoomed-out image within the output array\n","        zh = int(np.round(h * zoom_factor))\n","        zw = int(np.round(w * zoom_factor))\n","        top = (h - zh) // 2\n","        left = (w - zw) // 2\n","\n","        # Zero-padding\n","        out = np.zeros_like(img)\n","        out[top:top+zh, left:left+zw] = ndi.zoom(img, zoom_tuple, **kwargs)\n","\n","    # Zooming in\n","    elif zoom_factor > 1:\n","\n","        # Bounding box of the zoomed-in region within the input array\n","        zh = int(np.round(h / zoom_factor))\n","        zw = int(np.round(w / zoom_factor))\n","        top = (h - zh) // 2\n","        left = (w - zw) // 2\n","\n","        out = ndi.zoom(img[top:top+zh, left:left+zw], zoom_tuple, **kwargs)\n","\n","        # `out` might still be slightly larger than `img` due to rounding, so\n","        # trim off any extra pixels at the edges\n","        trim_top = ((out.shape[0] - h) // 2)\n","        trim_left = ((out.shape[1] - w) // 2)\n","        out = out[trim_top:trim_top+h, trim_left:trim_left+w]\n","\n","    # If zoom_factor == 1, just return the input array\n","    else:\n","        out = img\n","    return out"]},{"cell_type":"code","source":["def transformGridImage(sample, angle = None, zoom = None, shiftX = None, shiftY = None, \n","                        angleTransformChance = 0.1, zoomTransformChance = 0.1, shiftTransformChance = 0.1):\n","    brain_vol_data = sample.get_fdata()\n","    fig_rows = 4\n","    fig_cols = 4\n","    n_subplots = fig_rows * fig_cols\n","    n_slice = brain_vol_data.shape[2]\n","\n","    slices_to_eliminate = slicesToCut\n","\n","    n_slice_padding = slices_to_eliminate // 2 # quitamos los primeros y ultimos n slices\n","    n_slice = n_slice - slices_to_eliminate\n","\n","    step_size = n_slice / n_subplots\n","\n","    slice_indices = np.arange(n_slice_padding, n_slice_padding + n_slice, step = step_size)\n","\n","    fig, axs = plt.subplots(fig_rows, fig_cols, figsize=[10, 10], facecolor='black')\n","    \n","    if angle == None or angleTransformChance < random.uniform(0.0, 1.0):\n","        angle = 0.0 # Disable random angle\n","        \n","    if zoom != None and random.uniform(0.0, 1.0) > zoomTransformChance:\n","        zoom = None\n","        \n","    if shiftX != None and random.uniform(0.0, 1.0) > shiftTransformChance:\n","        shiftX = None\n","        \n","    if shiftY != None and random.uniform(0.0, 1.0) > shiftTransformChance:\n","        shiftY = None\n","\n","    idx = 0\n","    for img in slice_indices:\n","        processedImage = ndi.rotate(brain_vol_data[:, :, round(img)], 90.0 + angle)\n","        if zoom != None:\n","            processedImage = clipped_zoom(processedImage, zoom)\n","        if shiftX != None:\n","            processedImage = ndi.shift(processedImage, [0.0, shiftX, 0.0])\n","        if shiftY != None:\n","            processedImage = ndi.shift(processedImage, [shiftY, 0.0, 0.0])\n","        axs.flat[idx].imshow(np.squeeze(processedImage), cmap='gray')\n","        axs.flat[idx].axis('off')\n","        idx += 1\n","        \n","    plt.tight_layout()\n","\n","    image = fig2img(fig)\n","\n","    plt.close(fig) # Para que no muestre la imÃ¡gen\n","    \n","    return image"],"metadata":{"id":"2w0W-yXe-GRA","executionInfo":{"status":"ok","timestamp":1676057572574,"user_tz":180,"elapsed":7,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"vODiu92PQdIL","executionInfo":{"status":"ok","timestamp":1676057572574,"user_tz":180,"elapsed":7,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["# CreateGrid transform\n","class CreateGrid(object):\n","    \"\"\"Creates a grid from the image\n","    \"\"\"\n","    def __init__(self, transformArgs = {}):\n","        self.transformArgs = transformArgs\n","\n","    def __call__(self, sample):\n","        return transformGridImage(sample, **self.transformArgs)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"TX-5K8GGG-zT","executionInfo":{"status":"ok","timestamp":1676057572575,"user_tz":180,"elapsed":8,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["class RemoveTransparency(object):\n","    def __call__(self, sample):\n","      # La imagen se guarda con transparencia, removemos la dimension de indice 3\n","      return sample[0:3, :, :]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"UmM3wjZ0Qfa4","executionInfo":{"status":"ok","timestamp":1676057572575,"user_tz":180,"elapsed":8,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["class ToLabelOutput(object):\n","    def __call__(self, label):\n","        if label == \"CN\":\n","            return 0\n","        elif label == \"AD\":\n","            return 1\n","        else:\n","            return 2 # MCI, LMCI, EMCI"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"wxq4R1JOQgyU","executionInfo":{"status":"ok","timestamp":1676057572575,"user_tz":180,"elapsed":7,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["class ADNIDataset(Dataset):\n","    \"\"\"ADNI dataset.\"\"\"\n","\n","    def __init__(self, csv_file, root_dir, transform=None, target_transform = None, cacheSize = 200):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.csv = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        # file cache almacena las rutas a cada item\n","        self.filename_cache = [None] * len(self)\n","        # item_cache directamente almacena los items procesados\n","        self.cacheSize = cacheSize\n","        self.item_cache = [None] * cacheSize\n","\n","    def __len__(self):\n","      return int(len(self.csv))\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        if self.cacheSize > 0 and self.item_cache[idx % self.cacheSize] != None:\n","            item = self.item_cache[idx % self.cacheSize]\n","            if item[\"id\"] == idx:\n","                # item in cache\n","                return item[\"image\"], item[\"label\"]\n","            \n","        studyID = self.csv.iloc[idx, 0]\n","        subjectID = self.csv.iloc[idx, 1]\n","        processFormat = self.csv.iloc[idx, 7]\n","        date = self.csv.iloc[idx, 9]\n","        diagnosis = self.csv.iloc[idx, 2]\n","        \n","        filename = self.filename_cache[idx]\n","        \n","        if filename == None:\n","            rglob = '*'+str(studyID)+'*.nii'\n","            samples = 0\n","\n","            for path in Path(self.root_dir).rglob(rglob):\n","                filename = str(path)\n","                samples =+ 1\n","            \n","            if samples > 1:\n","                raise \"Mas de un sample. Error\"\n","\n","            self.filename_cache[idx] = filename\n","\n","        if not filename:\n","            raise Exception(\"Not found filename for index \" + str(idx) + \" y studyID \" + studyID)\n","            \n","        brain_vol = nib.load(filename)\n","\n","        image = brain_vol\n","        label = diagnosis\n","        \n","        if self.transform:\n","            image = self.transform(image)\n","            \n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        if self.cacheSize > 0 and self.item_cache[idx % self.cacheSize] == None:\n","            # Storing item in cache\n","            self.item_cache[idx % self.cacheSize] = {\n","                \"id\": idx,\n","                \"label\": label,\n","                \"image\": image\n","            }\n","\n","        return image, label"]},{"cell_type":"code","source":["def printFile(text, file):\n","  print(text)\n","  if file != None:\n","      file.write(text + \"\\n\")"],"metadata":{"id":"tMPKNDmqyALS","executionInfo":{"status":"ok","timestamp":1676057572575,"user_tz":180,"elapsed":7,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zW1D_vQnpzDk"},"source":["# Modelo"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"m44_ZbqJvsQ9","executionInfo":{"status":"ok","timestamp":1676057572576,"user_tz":180,"elapsed":8,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=True, logFile = None):\n","    f = None\n","    if logFile != None:\n","        f = open(logFile, \"w\")\n","\n","    since = time.time()\n","\n","    train_acc_history = []\n","    val_acc_history = []\n","    train_loss_history = []\n","    val_loss_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        printFile('Epoch {}/{}'.format(epoch, num_epochs - 1), f)\n","        printFile('-' * 10, f)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    if is_inception and phase == 'train':\n","                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n","                        # No usamos el aux\n","                        #outputs, aux_outputs = model(inputs)\n","                        #loss1 = criterion(outputs, labels)\n","                        #loss2 = criterion(aux_outputs, labels)\n","                        #loss = loss1 + 0.4*loss2\n","                        outputs, aux_outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            printFile('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), f)\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","                val_loss_history.append(epoch_loss)\n","            if phase == 'train':\n","                train_acc_history.append(epoch_acc)\n","                train_loss_history.append(epoch_loss)\n","            \n","\n","    time_elapsed = time.time() - since\n","    printFile('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60), f)\n","    printFile('Best val Acc: {:4f}'.format(best_acc), f)\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    if logFile != None:\n","        f.close()\n","    return model, val_acc_history, val_loss_history, train_acc_history, train_loss_history"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Vl-UuMFJv0_S","executionInfo":{"status":"ok","timestamp":1676057572576,"user_tz":180,"elapsed":8,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"]},{"cell_type":"markdown","metadata":{"id":"DuMUC0zqwFLw"},"source":["# Initialize and reshape inception"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":384,"status":"ok","timestamp":1676057572953,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"},"user_tz":180},"id":"GqZfuwYwwLw2","outputId":"d1363366-7a57-46d2-db61-e3d118a6fb84"},"outputs":[{"output_type":"stream","name":"stdout","text":["num featurs2048\n"]}],"source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"inception\":\n","        \"\"\" Inception v3\n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained, \n","                                       aux_logits = True)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        # num_ftrs = model_ft.AuxLogits.fc.in_features\n","        # model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        print(\"num featurs\" + str(num_ftrs))\n","        # Fuente: https://github.com/bdrad/petdementiapub/blob/master/petdementia_source.py\n","        model_ft.fc = nn.Sequential(\n","          nn.Linear(num_ftrs,1024),\n","          nn.ReLU(),\n","          nn.Linear(1024,num_classes),\n","        )\n","          \n","        input_size = 512 \n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","\n","    return model_ft, input_size\n","\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=usePretrained)\n","\n","# Print the model we just instantiated\n","# print(model_ft)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":707,"status":"ok","timestamp":1676057573658,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"},"user_tz":180},"id":"pfACeWNcwYqY","outputId":"aea10226-085c-4551-e651-e35fe84492c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing Datasets and Dataloaders...\n"]}],"source":["# Data augmentation and normalization for training\n","# Just normalization for validation\n","data_transforms = {\n","    'train': transforms.Compose([\n","        CreateGrid(dataAugmentation),\n","        transforms.ToTensor(),\n","        RemoveTransparency(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        CreateGrid(),\n","        transforms.ToTensor(),\n","        RemoveTransparency(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","print(\"Initializing Datasets and Dataloaders...\")\n","\n","# Create training and validation datasets\n","\n","image_datasets = {\n","    'train': ADNIDataset(trainDatasetCSV, imagesFolder, transform = data_transforms['train'], target_transform =ToLabelOutput(), cacheSize = trainCacheSize ),\n","    'val': ADNIDataset(valDatasetCSV, imagesFolder, transform = data_transforms['val'], target_transform =ToLabelOutput(), cacheSize = validationCacheSize )\n","}\n","\n","# Create training and validation dataloaders\n","dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n","\n","# Detect if we have a GPU available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2208,"status":"ok","timestamp":1676057575865,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"},"user_tz":180},"id":"XciJ190PwerB","outputId":"de48ccc9-3426-4e68-8c24-89c86687c3fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Params to learn:\n","\t fc.0.weight\n","\t fc.0.bias\n","\t fc.2.weight\n","\t fc.2.bias\n"]}],"source":["# Send the model to GPU\n","model_ft = model_ft.to(device)\n","\n","# Gather the parameters to be optimized/updated in this run. If we are\n","#  finetuning we will be updating all parameters. However, if we are\n","#  doing feature extract method, we will only update the parameters\n","#  that we have just initialized, i.e. the parameters with requires_grad\n","#  is True.\n","params_to_update = model_ft.parameters()\n","print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.Adam(params_to_update, lr=learningRate)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"CVJl5AvUqK13","executionInfo":{"status":"ok","timestamp":1676057575865,"user_tz":180,"elapsed":5,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"outputs":[],"source":["def test_model(model,dataloaders,device):\n","    classStats = [{\n","        'fn': 0,\n","        'tn': 0,\n","        'tp': 0,\n","        'fp': 0,\n","        'n': 0,\n","    } for i in range(num_classes)]\n","    correctlyPredicted = 0\n","    n = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for inputs, labels in dataloaders['val']:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            \n","            # Iteramos para chequear estadisticas\n","            for i, correctClass in enumerate(labels.data):\n","              n += 1\n","              predictedClass = int(preds[i].item())\n","              correctClass = int(correctClass.item())\n","              classStats[correctClass]['n'] += 1\n","              if correctClass == predictedClass:\n","                  correctlyPredicted += 1\n","                  classStats[correctClass]['tp'] += 1\n","                  for i in range(num_classes):\n","                      if i != correctClass:\n","                          classStats[correctClass]['tn'] += 1\n","              else:\n","                  classStats[correctClass]['fn'] += 1\n","                  classStats[predictedClass]['fp'] += 1\n","                  for i in range(num_classes):\n","                      if i != correctClass and i != predictedClass:\n","                          classStats[correctClass]['tn'] += 1\n","    accuracy = correctlyPredicted * 1.0 / n\n","    return classStats, accuracy"]},{"cell_type":"code","source":["def printClassStats(stats):\n","  recall = sensitivity = stats['tp'] / (stats['tp'] + stats['fn']) # prob positive test result\n","  specificity = stats['tn'] / (stats['tn'] + stats['fp'])          # prob negative test result\n","  if stats['tp'] + stats['fp'] > 0:\n","    precision = stats['tp'] / (stats['tp'] + stats['fp'])          # prob of recognized positive actually correct\n","  else:\n","    precision = 1\n","    printFile(\"Setting precision as 1 but no positive value has been reported, so this is placeholder\", f)\n","  if precision + recall == 0:\n","    printFile(\"Setting f1 as 0 because precision + recall is ZERO\", f)\n","    f1 = 0.0\n","  else:\n","    f1 = 2 * (precision * recall) / ( precision + recall )\n","  printFile(\"Sensitivity (%): \" + str(round(sensitivity * 100)), f)\n","  printFile(\"Specificity (%): \" + str(round(specificity * 100)), f)\n","  printFile(\"Precision  (%): \" + str(round(precision * 100)), f)\n","  printFile(\"F1 Score  (%): \" + str(round(f1 * 100)), f)\n","  printFile(\"Number of images: \" + str(stats['n']), f)\n","  return recall, specificity, precision, f1"],"metadata":{"id":"aK3auwSS05Le","executionInfo":{"status":"ok","timestamp":1676057575865,"user_tz":180,"elapsed":5,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"D84am68jwlX6","executionInfo":{"status":"ok","timestamp":1676067120188,"user_tz":180,"elapsed":600315,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}},"outputId":"134a201b-405c-40f0-ddcc-561ec50d9ab7"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Execution 0 begin ---\n","Epoch 0/49\n","----------\n","train Loss: 1.1138 Acc: 0.3429\n","val Loss: 1.0936 Acc: 0.3467\n","Epoch 1/49\n","----------\n","train Loss: 1.0805 Acc: 0.4111\n","val Loss: 1.0902 Acc: 0.3633\n","Epoch 2/49\n","----------\n","train Loss: 1.0854 Acc: 0.3857\n","val Loss: 1.1180 Acc: 0.3700\n","Epoch 3/49\n","----------\n","train Loss: 1.0832 Acc: 0.3746\n","val Loss: 1.0890 Acc: 0.3767\n","Epoch 4/49\n","----------\n","train Loss: 1.0478 Acc: 0.4524\n","val Loss: 1.0886 Acc: 0.3933\n","Epoch 5/49\n","----------\n","train Loss: 1.0383 Acc: 0.4444\n","val Loss: 1.0898 Acc: 0.3967\n","Epoch 6/49\n","----------\n","train Loss: 1.0614 Acc: 0.4143\n","val Loss: 1.0872 Acc: 0.3833\n","Epoch 7/49\n","----------\n","train Loss: 1.0476 Acc: 0.4635\n","val Loss: 1.1174 Acc: 0.3967\n","Epoch 8/49\n","----------\n","train Loss: 1.0321 Acc: 0.4762\n","val Loss: 1.0913 Acc: 0.4067\n","Epoch 9/49\n","----------\n","train Loss: 1.0227 Acc: 0.4952\n","val Loss: 1.1134 Acc: 0.4133\n","Epoch 10/49\n","----------\n","train Loss: 1.0213 Acc: 0.4730\n","val Loss: 1.1068 Acc: 0.3900\n","Epoch 11/49\n","----------\n","train Loss: 1.0198 Acc: 0.4746\n","val Loss: 1.0883 Acc: 0.4200\n","Epoch 12/49\n","----------\n","train Loss: 1.0090 Acc: 0.5111\n","val Loss: 1.0895 Acc: 0.4167\n","Epoch 13/49\n","----------\n","train Loss: 0.9873 Acc: 0.4984\n","val Loss: 1.0998 Acc: 0.4233\n","Epoch 14/49\n","----------\n","train Loss: 0.9806 Acc: 0.5222\n","val Loss: 1.0966 Acc: 0.4300\n","Epoch 15/49\n","----------\n","train Loss: 1.0083 Acc: 0.4952\n","val Loss: 1.1179 Acc: 0.4233\n","Epoch 16/49\n","----------\n","train Loss: 0.9777 Acc: 0.5159\n","val Loss: 1.1086 Acc: 0.3967\n","Epoch 17/49\n","----------\n","train Loss: 0.9644 Acc: 0.5302\n","val Loss: 1.1065 Acc: 0.3900\n","Epoch 18/49\n","----------\n","train Loss: 0.9754 Acc: 0.5079\n","val Loss: 1.0999 Acc: 0.4000\n","Epoch 19/49\n","----------\n","train Loss: 0.9612 Acc: 0.5460\n","val Loss: 1.1095 Acc: 0.3933\n","Epoch 20/49\n","----------\n","train Loss: 0.9621 Acc: 0.5476\n","val Loss: 1.1106 Acc: 0.3800\n","Epoch 21/49\n","----------\n","train Loss: 0.9832 Acc: 0.5254\n","val Loss: 1.1226 Acc: 0.3667\n","Epoch 22/49\n","----------\n","train Loss: 0.9535 Acc: 0.5381\n","val Loss: 1.1171 Acc: 0.3833\n","Epoch 23/49\n","----------\n","train Loss: 0.9576 Acc: 0.5413\n","val Loss: 1.1526 Acc: 0.3800\n","Epoch 24/49\n","----------\n","train Loss: 0.9651 Acc: 0.5222\n","val Loss: 1.1379 Acc: 0.4033\n","Epoch 25/49\n","----------\n","train Loss: 0.9612 Acc: 0.5365\n","val Loss: 1.1507 Acc: 0.4033\n","Epoch 26/49\n","----------\n","train Loss: 0.9484 Acc: 0.5413\n","val Loss: 1.1288 Acc: 0.3733\n","Epoch 27/49\n","----------\n","train Loss: 0.9348 Acc: 0.5524\n","val Loss: 1.1416 Acc: 0.3733\n","Epoch 28/49\n","----------\n","train Loss: 0.9432 Acc: 0.5349\n","val Loss: 1.1248 Acc: 0.3800\n","Epoch 29/49\n","----------\n","train Loss: 0.9258 Acc: 0.5397\n","val Loss: 1.1177 Acc: 0.4100\n","Epoch 30/49\n","----------\n","train Loss: 0.9353 Acc: 0.5540\n","val Loss: 1.2245 Acc: 0.3500\n","Epoch 31/49\n","----------\n","train Loss: 0.9729 Acc: 0.5413\n","val Loss: 1.1437 Acc: 0.3933\n","Epoch 32/49\n","----------\n","train Loss: 0.9292 Acc: 0.5524\n","val Loss: 1.1200 Acc: 0.3900\n","Epoch 33/49\n","----------\n","train Loss: 0.9225 Acc: 0.5667\n","val Loss: 1.1423 Acc: 0.3467\n","Epoch 34/49\n","----------\n","train Loss: 0.9427 Acc: 0.5508\n","val Loss: 1.1520 Acc: 0.4133\n","Epoch 35/49\n","----------\n","train Loss: 0.9386 Acc: 0.5841\n","val Loss: 1.1525 Acc: 0.3433\n","Epoch 36/49\n","----------\n","train Loss: 0.9263 Acc: 0.5619\n","val Loss: 1.1426 Acc: 0.3900\n","Epoch 37/49\n","----------\n","train Loss: 0.9141 Acc: 0.5476\n","val Loss: 1.1486 Acc: 0.3700\n","Epoch 38/49\n","----------\n","train Loss: 0.9552 Acc: 0.5524\n","val Loss: 1.1728 Acc: 0.3933\n","Epoch 39/49\n","----------\n","train Loss: 0.9093 Acc: 0.5825\n","val Loss: 1.1428 Acc: 0.3800\n","Epoch 40/49\n","----------\n","train Loss: 0.9409 Acc: 0.5619\n","val Loss: 1.1675 Acc: 0.3567\n","Epoch 41/49\n","----------\n","train Loss: 0.9211 Acc: 0.5603\n","val Loss: 1.1764 Acc: 0.3500\n","Epoch 42/49\n","----------\n","train Loss: 0.9179 Acc: 0.5746\n","val Loss: 1.1681 Acc: 0.3667\n","Epoch 43/49\n","----------\n","train Loss: 0.9342 Acc: 0.5540\n","val Loss: 1.1837 Acc: 0.4133\n","Epoch 44/49\n","----------\n","train Loss: 0.9414 Acc: 0.5476\n","val Loss: 1.1633 Acc: 0.3433\n","Epoch 45/49\n","----------\n","train Loss: 0.9372 Acc: 0.5540\n","val Loss: 1.1649 Acc: 0.3700\n","Epoch 46/49\n","----------\n","train Loss: 0.9180 Acc: 0.5698\n","val Loss: 1.1595 Acc: 0.4167\n","Epoch 47/49\n","----------\n","train Loss: 0.9139 Acc: 0.5714\n","val Loss: 1.1465 Acc: 0.3967\n","Epoch 48/49\n","----------\n","train Loss: 0.9093 Acc: 0.5444\n","val Loss: 1.2339 Acc: 0.4067\n","Epoch 49/49\n","----------\n","train Loss: 0.9168 Acc: 0.5746\n","val Loss: 1.1438 Acc: 0.3700\n","Training complete in 157m 59s\n","Best val Acc: 0.430000\n","accuracy: 0.43\n","CN stats: \n","Sensitivity (%): 49\n","Specificity (%): 69\n","Precision  (%): 42\n","F1 Score  (%): 45\n","Number of images: 100\n","\n","AD stats: \n","Sensitivity (%): 43\n","Specificity (%): 78\n","Precision  (%): 51\n","F1 Score  (%): 47\n","Number of images: 100\n","\n","MCI stats: \n","Sensitivity (%): 37\n","Specificity (%): 69\n","Precision  (%): 37\n","F1 Score  (%): 37\n","Number of images: 100\n","--- Execution End ---\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}}],"source":["accuracyValues = []\n","adStatValues = []\n","cnStatValues = []\n","mciStatValues = []\n","for i in range(0, executions):\n","    experimentExecutionName = experimentName + '_' + str(i)\n","    print(\"--- Execution \" + str(i) + \" begin ---\")\n","    # Setup the loss fxn\n","    crossEntrophyWeigths = crossEntrophyWeigths.to(device)\n","    criterion = nn.CrossEntropyLoss(crossEntrophyWeigths)\n","\n","    logFile = os.path.join(experimentOutputFolder, experimentExecutionName + '_train.log')\n","\n","    # Train and evaluate\n","    model_ft, val_acc_hist, val_loss_hist, train_acc_hist, train_loss_hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"), logFile = logFile)\n","\n","    torch.save(model_ft.state_dict(), os.path.join(experimentOutputFolder, experimentExecutionName + '.pth'))\n","\n","    # validation accuracy\n","    fig = plt.figure()\n","    lst = [ x.cpu().item() for x in val_acc_hist ]\n","    plt.plot(lst)\n","    ax = plt.gca()\n","    plt.text(0.05, 0.9, 'FE = ' + str(feature_extract), transform=ax.transAxes)\n","    plt.text(0.05, 0.8, 'LR = ' + str(learningRate), transform=ax.transAxes)\n","    plt.text(0.05, 0.7, 'batch = ' + str(batch_size), transform=ax.transAxes)\n","    plt.suptitle(experimentExecutionName + ' (acc set de validacion)')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.savefig(os.path.join(experimentOutputFolder, experimentExecutionName + '_val_acc.png'))\n","    plt.clf()\n","\n","    # validation loss\n","    fig = plt.figure()\n","    plt.plot(val_loss_hist)\n","    ax = plt.gca()\n","    plt.text(0.05, 0.3, 'FE = ' + str(feature_extract), transform=ax.transAxes)\n","    plt.text(0.05, 0.2, 'LR = ' + str(learningRate), transform=ax.transAxes)\n","    plt.text(0.05, 0.1, 'batch = ' + str(batch_size), transform=ax.transAxes)\n","    plt.suptitle(experimentExecutionName + ' (loss set de validacion)')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epochs')\n","    plt.savefig(os.path.join(experimentOutputFolder, experimentExecutionName + '_val_loss.png'))\n","    plt.clf()\n","\n","    # train accuracy\n","    fig = plt.figure()\n","    lst = [ x.cpu().item() for x in train_acc_hist ]\n","    ax = plt.gca()\n","    plt.text(0.05, 0.9, 'FE = ' + str(feature_extract), transform=ax.transAxes)\n","    plt.text(0.05, 0.8, 'LR = ' + str(learningRate), transform=ax.transAxes)\n","    plt.text(0.05, 0.7, 'batch = ' + str(batch_size), transform=ax.transAxes)\n","    plt.plot(lst)\n","    plt.suptitle(experimentExecutionName + ' (accuracy set de train)')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.savefig(os.path.join(experimentOutputFolder, experimentExecutionName + '_train_acc.png'))\n","    plt.clf()\n","\n","    # train loss\n","    fig = plt.figure()\n","    ax = plt.gca()\n","    plt.text(0.05, 0.3, 'FE = ' + str(feature_extract), transform=ax.transAxes)\n","    plt.text(0.05, 0.2, 'LR = ' + str(learningRate), transform=ax.transAxes)\n","    plt.text(0.05, 0.1, 'batch = ' + str(batch_size), transform=ax.transAxes)\n","    plt.plot(train_loss_hist)\n","    plt.suptitle(experimentExecutionName + ' (Loss set de train)')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epochs')\n","    plt.savefig(os.path.join(experimentOutputFolder, experimentExecutionName + '_train_loss.png'))\n","    plt.clf()\n","\n","    stats, accuracy = test_model(model_ft, dataloaders_dict, device)\n","\n","    print(\"accuracy: \" + str(accuracy))\n","    accuracyValues.append(accuracy)\n","\n","    f = open(os.path.join(experimentOutputFolder, experimentExecutionName + \"_stats.txt\"), \"w\")\n","    printFile(\"CN stats: \", f)\n","    recall, specificity, precision, f1 = printClassStats(stats[0])\n","    cnStatValues.append({\n","        \"recall\": recall,\n","        \"specificity\": specificity,\n","        \"precision\": precision,\n","        \"f1\": f1\n","    })\n","    # AD\n","    printFile(\"\\nAD stats: \", f)\n","    recall, specificity, precision, f1 = printClassStats(stats[1])\n","    adStatValues.append({\n","        \"recall\": recall,\n","        \"specificity\": specificity,\n","        \"precision\": precision,\n","        \"f1\": f1\n","    })\n","    # MCI\n","    printFile(\"\\nMCI stats: \", f)\n","    recall, specificity, precision, f1 = printClassStats(stats[2])\n","    mciStatValues.append({\n","        \"recall\": recall,\n","        \"specificity\": specificity,\n","        \"precision\": precision,\n","        \"f1\": f1\n","    })\n","    f.close()\n","\n","    print(\"--- Execution End ---\")"]},{"cell_type":"code","source":["accuracyValues = torch.tensor(accuracyValues)\n","std, mean = torch.std_mean(accuracyValues)\n","f = open(os.path.join(\n","    os.path.join(experimentOutputFolder, experimentName + '_results.txt')), \"w\")\n","printFile(\"Final stats: \", f)\n","printFile(\"Executions: \" + str(executions), f)\n","printFile(\"Accuracy mean: \" + str(mean.item()), f)\n","printFile(\"Accuracy std: \" + str(std.item()), f)\n","printFile(\"Best accuracy: \" + str(accuracyValues.max().item()), f)\n","printFile(\"Worst accuracy: \" + str(accuracyValues.min().item()), f)\n","f.close()"],"metadata":{"id":"WuiTfuCMd8j4","executionInfo":{"status":"ok","timestamp":1676067120195,"user_tz":180,"elapsed":1,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a799a5b-9067-43b0-bbb8-f4d3d5314732"},"execution_count":24,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Final stats: \n","Executions: 1\n","Accuracy mean: 0.4300000071525574\n","Accuracy std: nan\n","Best accuracy: 0.4300000071525574\n","Worst accuracy: 0.4300000071525574\n"]}]},{"cell_type":"markdown","source":["# ROC - AUC"],"metadata":{"id":"xA-ShjPRUYxP"}},{"cell_type":"code","source":["def sortByX(point):\n","  return point[0]\n","\n","# Se necesita ordenar los puntos para llamar a torch.trapezoid con los puntos ordenados por la coordenada X\n","def sortListsByX(x, y):\n","    points = zip(x, y)\n","    points = list(points)\n","    points.sort(key=sortByX)\n","    x, y = ([ a for a,b in points ], [ b for a,b in points ])\n","    return x, y\n","\n","fig = plt.figure()\n","ad_y = ad_tpr = [ x[\"recall\"] for x in adStatValues ]\n","ad_x = ad_fpr = [ 1.0 - x[\"specificity\"] for x in adStatValues ]\n","ad_x, ad_y = sortListsByX(ad_x, ad_y)\n","\n","cn_y = cn_tpr = [ x[\"recall\"] for x in cnStatValues ]\n","cn_x = cn_fpr = [ 1.0 - x[\"specificity\"] for x in cnStatValues ]\n","cn_x, cn_y = sortListsByX(cn_x, cn_y)\n","\n","mci_y = mci_tpr = [ x[\"recall\"] for x in mciStatValues ]\n","mci_x = mci_fpr = [ 1.0 - x[\"specificity\"] for x in mciStatValues ]\n","mci_x, mci_y = sortListsByX(mci_x, mci_y)\n","\n","decimals = 3\n","ad_auc = round(torch.trapezoid(torch.tensor(ad_y), torch.tensor(ad_x)).item(), decimals)\n","cn_auc = round(torch.trapezoid(torch.tensor(cn_y), torch.tensor(cn_x)).item(), decimals)\n","mci_auc = round(torch.trapezoid(torch.tensor(mci_y), torch.tensor(mci_x)).item(), decimals)\n","\n","plt.title(experimentName + \" ROC\")\n","\n","plt.plot(ad_fpr, ad_tpr)\n","plt.plot(cn_fpr, cn_tpr)\n","plt.plot(mci_fpr, mci_tpr)\n","\n","plt.legend([\"AD (\"+str(ad_auc)+\")\", \"CN (\"+str(cn_auc)+\")\", \"MCI (\"+str(mci_auc)+\")\"], loc =\"lower right\")\n","\n","plt.show()"],"metadata":{"id":"mmKtfcWNUOIR","executionInfo":{"status":"ok","timestamp":1676067120190,"user_tz":180,"elapsed":4,"user":{"displayName":"Hugo Massaroli","userId":"15440061811672054235"}},"colab":{"base_uri":"https://localhost:8080/","height":281},"outputId":"0c01090d-a163-4119-eb03-37c4b2ae7046"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1wBDyx6qBbMVKhBkalmDDh8fHPGizULqY","timestamp":1675975075129},{"file_id":"15DQCaEowifBeL7LrHIhH27T1wLMqux-Q","timestamp":1675862573434},{"file_id":"1yGoLeZe4i8JaHXU1IfjR2UPRsLtU23F9","timestamp":1675797870814},{"file_id":"120K10bTpyu0fdEdMVM__L0qERi9n-jAS","timestamp":1675439211225},{"file_id":"13WvbakefqyInUNu3IQON4PMLfeqtRAfy","timestamp":1674663309852},{"file_id":"1iT4xx9DO_8Lc0tKgVBhlpCz5eWSt06ey","timestamp":1674653538354},{"file_id":"1ZK41MNgZDEE9pw_LuWBGh9zFE2lC1Nc9","timestamp":1674333964346},{"file_id":"1pM4FyjV72HeYXVll7Q9TokiVunh0UijY","timestamp":1674152895718}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}